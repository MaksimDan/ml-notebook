{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Machine learning provides system the capability to learn and improve from example, without the need of any explicit programming. This provides the means of approximating sense from data - with the aim of then generalizing it in order to make predictions. This means good outside performance must be made beyond training data. There is a lot of black art involved and the selection of an algorithm isn't always clearly communicated. There tends to be a lot of guess work and twiddling of nobs \\(parameters\\) to tune a learner.\n",
    "\n",
    "There are thousands of machine learning algorithms but every algorithm falls under the combination of three components:\n",
    "\n",
    "1. Representation: How is the data represented to the learner? This determines what classifiers can be used.\n",
    "2. Evaluation: We need a mathematical way to distinguish good classifiers from bad ones. This is what the evaluation or objective function does. Often times evaluation functions produce more than one Optima - which in turn help in determining the kind of classifier to use.\n",
    "3. Optimization: Finally, we need a way to search among all classifiers and determine the best one. This choice of optimization is paramount to the efficiency of the learner\n",
    "\n",
    "All learners essentially work by grouping nearby classes into the same class. The only difference is their definition of 'nearby'.\n",
    "\n",
    "**Data**\n",
    "\n",
    "Generally, the more data - the better. But there is almost never enough - because the space of combinations will almost always outweigh a number of examples. Therefore machine learning algorithm has to generalize by making a few assumptions in order to be better than random guess work. This was formalized by Wolpert in his famous \"no free lunch theorem\", which state that no machine algorithm is the universally best algorithm. In otherwords, there is no \"one-size-fits-all\" algorithm.  Instead, the outcome of data is more probabilistic in nature, it is good practice to try more than one fine-tuned model. However, examples are not always uniformly draw from random from the complete space of possibilities - and this is generally enough for machine learning algorithms to produce more from less.\n",
    "\n",
    "**How large should your training data be?**\n",
    "\n",
    "This depends largely on the amount of data you have available. With large data, it is entirely possible to become to fit the best model you can get with just 10% of the data, and then use the remaining data on validation. With small data, you have to be more conservative and generally more data is conserved for training. This is because training is the hardest and most important part. Consider the search space of all possible configurations of a decision tree is a lot larger than the space of possible pruning options because pruning comes out after having to learn the data. There more technique to use such as with _k-fold cross validation_, we can utalizes both train and test sets for validation.\n",
    "\n",
    "**Over fitting**\n",
    "\n",
    "Not having enough data means running the risk of deluding a classifier, or over fitting. When this happens, the algorithm performs with high accuracy on a training set, but not real test data. One way it is understood is through is by decomposing generalization error of bias and variance.\n",
    "\n",
    "**under-fitting**: the tendency to learn the same thing incorrectly. Example: applying a linear classifier on a system where two classes are ill-segmented by a hyperplane.\n",
    "\n",
    "**Variance \\(over-fitting\\)**: the tendency to learn random things irrespective of the real thing. Example: decision trees that produce different behavior on data sets involving the same phenomena.\n",
    "\n",
    "**Combating Overfitting**\n",
    "\n",
    "* More data.\n",
    "* Adding regularization term to evaluation function \\(thereby penalizing classifier that has more structure\\).\n",
    "\n",
    "There is no single technique that can be used to avoid over-fitting - but there are ways to help minimize it.\n",
    "\n",
    "**Curse of Dimensionality**\n",
    "\n",
    "Generalizing in high dimensions become exponentially more difficult - since the features we have the more intractable it becomes. High dimensional inputs mean we need more data because the feature spaces become exponentially more vast. Additionally, human intuition breaks down in any dimension after the 3rd. Brades and tangles in the 4th dimension, for example, are able to pass through each other. Every additional dimension adds something that may have felt intuitively impossible from before. Some say that machine learning would be necessary if it were possible to both visually and intuitively inspect dimensions &gt; 3.\n",
    "\n",
    "On the bright side, many machine learning applications are not uniformly drawn from the feature space \\(\"blessing of non-uniformity\"\\), but are rather just concentrated near a lower dimension manifold. Take digit recognition, for example, the space for all possible images are incredibly large, but digits will generally take a small approximate space of this - so that learners can take advantage of this effectively lower dimension.\n",
    "\n",
    "**What makes a successful machine learning algorithm**\n",
    "\n",
    "The selection of features. If you have many independent features that correlate well with the class, then learning is easy. Sometimes it possible to select features based on the output of the best classifier produced based on the combination of features selected. One problem with this approach is that it may cause overfitting or be too time-consuming. Another problem is that data isn't often in the form that is amenable to learning - so a lot of time is generally spent gathering, integrating, reconfiguring, reformatting, cleaning and preprocessing it.\n",
    "\n",
    "It increases the effectiveness of our results we have two options: provide more data or use a better algorithm. Pragmatically, it is quicker to just use more data. One problem with this is that even though this is a lot of data in the world today, a lot of it goes unused by complex classifiers because these simply take up too much time to learn. Generally - start with the simpler algorithms and move towards the complex.\n",
    "\n",
    "Different learners will behave better or worse depending on the application. Often the combination of different classifiers performs even better when put together. In a technique known as bagging, random data is generated from variations of the training set through resampling. Then a classifier is learned on each, the results are combined to bring the most favorable decision through voting. In boosting, training examples have weights and within each iteration of the algorithm these weights become shifted so that the classifiers focus on that one that was previously wrong.\n",
    "\n",
    "**Simplicity and Accuracy**\n",
    "\n",
    "Occums razor states that entities would not get multiplied beyond necessity. In other words, given two explanations of equal possibility, the simpler one is often the correct one. Accums razor is a heuristic followed that suggests starting with the simplest solution is and move towards the more complex. In machine learning, if two classifiers have the same training error, the simpler one will generally do better on test data. However, there are several counter examples - such as model assembling discussed earlier, or SVM's and parameter tuning.\n",
    "\n",
    "**Correlation and Causation**\n",
    "\n",
    "Even though machine learning algorithms identify with correlations - they are still used because the goal of predictive modeling is to use them as profitable guides to action. Correlation often gives a direction on where to explore next in the causal chain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WorkFlow\n",
    "\n",
    "Every machine learning paradigm follows the same basic structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"415pt\" height=\"392pt\"\n",
       " viewBox=\"0.00 0.00 414.50 392.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 388)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-388 410.5,-388 410.5,4 -4,4\"/>\n",
       "<!-- Start -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>Start</title>\n",
       "<polygon fill=\"#b2dfee\" stroke=\"#b2dfee\" points=\"182.5,-384 128.5,-384 128.5,-348 182.5,-348 182.5,-384\"/>\n",
       "<text text-anchor=\"middle\" x=\"155.5\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Start</text>\n",
       "</g>\n",
       "<!-- Data Preparation -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Data Preparation</title>\n",
       "<polygon fill=\"#b2dfee\" stroke=\"#b2dfee\" points=\"211,-297 100,-297 100,-261 211,-261 211,-297\"/>\n",
       "<text text-anchor=\"middle\" x=\"155.5\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Data Preparation</text>\n",
       "</g>\n",
       "<!-- Start&#45;&gt;Data Preparation -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>Start&#45;&gt;Data Preparation</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M155.5,-347.9735C155.5,-336.1918 155.5,-320.5607 155.5,-307.1581\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"159.0001,-307.0033 155.5,-297.0034 152.0001,-307.0034 159.0001,-307.0033\"/>\n",
       "<text text-anchor=\"middle\" x=\"187.5\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">data source</text>\n",
       "</g>\n",
       "<!-- Training Data -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Training Data</title>\n",
       "<polygon fill=\"#b2dfee\" stroke=\"#b2dfee\" points=\"203,-210 108,-210 108,-174 203,-174 203,-210\"/>\n",
       "<text text-anchor=\"middle\" x=\"155.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Training Data</text>\n",
       "</g>\n",
       "<!-- Data Preparation&#45;&gt;Training Data -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Data Preparation&#45;&gt;Training Data</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M155.5,-260.9735C155.5,-249.1918 155.5,-233.5607 155.5,-220.1581\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"159.0001,-220.0033 155.5,-210.0034 152.0001,-220.0034 159.0001,-220.0033\"/>\n",
       "<text text-anchor=\"middle\" x=\"281\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">data transformation, cleaning and preprocess</text>\n",
       "</g>\n",
       "<!-- Algorithm Training, and Optimial Model Evaluation -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>Algorithm Training, and Optimial Model Evaluation</title>\n",
       "<polygon fill=\"#b2dfee\" stroke=\"#b2dfee\" points=\"311,-123 0,-123 0,-87 311,-87 311,-123\"/>\n",
       "<text text-anchor=\"middle\" x=\"155.5\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Algorithm Training, and Optimial Model Evaluation</text>\n",
       "</g>\n",
       "<!-- Training Data&#45;&gt;Algorithm Training, and Optimial Model Evaluation -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>Training Data&#45;&gt;Algorithm Training, and Optimial Model Evaluation</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M155.5,-173.9735C155.5,-162.1918 155.5,-146.5607 155.5,-133.1581\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"159.0001,-133.0033 155.5,-123.0034 152.0001,-133.0034 159.0001,-133.0033\"/>\n",
       "<text text-anchor=\"middle\" x=\"207.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">clean training data</text>\n",
       "</g>\n",
       "<!-- Deployment and Monitoring -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Deployment and Monitoring</title>\n",
       "<polygon fill=\"#b2dfee\" stroke=\"#b2dfee\" points=\"244,-36 67,-36 67,0 244,0 244,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"155.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Deployment and Monitoring</text>\n",
       "</g>\n",
       "<!-- Algorithm Training, and Optimial Model Evaluation&#45;&gt;Deployment and Monitoring -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>Algorithm Training, and Optimial Model Evaluation&#45;&gt;Deployment and Monitoring</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M155.5,-86.9735C155.5,-75.1918 155.5,-59.5607 155.5,-46.1581\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"159.0001,-46.0033 155.5,-36.0034 152.0001,-46.0034 159.0001,-46.0033\"/>\n",
       "<text text-anchor=\"middle\" x=\"196.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">optimal model</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x111f64198>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "g = Digraph('G')\n",
    "\n",
    "g.attr('node', shape='box')\n",
    "g.node_attr.update(color='lightblue2', style='filled')\n",
    "g.edge('Start', 'Data Preparation', 'data source')\n",
    "g.edge('Data Preparation', 'Training Data', 'data transformation, cleaning and preprocess')\n",
    "g.edge('Training Data', 'Algorithm Training, and Optimial Model Evaluation', 'clean training data')\n",
    "g.edge('Algorithm Training, and Optimial Model Evaluation', 'Deployment and Monitoring', 'optimal model')\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
