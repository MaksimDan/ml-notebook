{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra Review\n",
    "\n",
    "## Rank\n",
    "\n",
    "The _rank_ of a matrix is the dimension of the vector space spanned from its columns. It is in other words the maximum number of linearly indpendent columns in a matrix. A column is linearly independent if it cannot be written as a linear combination of the other columns.\n",
    "\n",
    "As an example, suppose we have the following matrix\n",
    "\n",
    "$$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "    1 & 0 & -1 & 0 & 4 \\\\\n",
    "    2 & 1 & 0 & 0 & 9 \\\\\n",
    "    -1 & 2 & 5 & 1 & -5 \\\\\n",
    "    1 & -1 & -3 & -2 & 9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The first step in determining its rank is to reduce the matrix into reduced row echelon form. The purpose of RREF is to represent the matrix in such as way the properities can be easily identified from the matrix. This is acheived by reducing the matrix as much as possible by only using the linear combination of other rows. More specifically, RREF demands that (1) all zero rows, if any, belong at the bottom of the matrix, and (2) the first nonzero number from the left of a nonzero row is always strictly to the right of the leading coefficient of the row above it. \n",
    "\n",
    "In our case, the progression for $A$ might looks like:\n",
    "\n",
    "$$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "    1 & 0 & -1 & 0 & 4 \\\\\n",
    "    2 & 1 & 0 & 0 & 9 \\\\\n",
    "    -1 & 2 & 5 & 1 & -5 \\\\\n",
    "    1 & -1 & -3 & -2 & 9\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "    1 & 0 & -1 & 0 & 4 \\\\\n",
    "    0 & 1 & 2 & 0 & 1 \\\\\n",
    "    0 & 2 & 4 & 1 & -1 \\\\\n",
    "    1 & -1 & -2 & -2 & 5\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    1 & 0 & -1 & 0 & 4 \\\\\n",
    "    0 & 1 & 2 & 0 & 1 \\\\\n",
    "    0 & 0 & 0 & 1 & -3 \\\\\n",
    "    0 & 0 & 0 & -2 & 6\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    1 & 0 & -1 & 0 & 4 \\\\\n",
    "    0 & 1 & 2 & 0 & 1 \\\\\n",
    "    0 & 0 & 0 & 1 & -3 \\\\\n",
    "    0 & 0 & 0 & 0 & 0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In this example, we've reduced $A$ into 3 rows and effectively eliminated the 4th row, because it could be redundently composted by the other columns. In particular, columns 1,2 and 4 in both the RREF and original matrix have pivot entries. These 3 columns are strictly independent because in no way can they can be canceled out by the remaining columns, the zeros will just cancel or zero out any attempt to do so.\n",
    "\n",
    "$$\n",
    "A_r = \n",
    "\\begin{bmatrix}\n",
    "    1 & 0 & 0 \\\\\n",
    "    0 & 1 & 0 \\\\\n",
    "    0 & 0 & 1  \\\\\n",
    "    0 & 0 & 0 \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    1 & 0 & 0 \\\\\n",
    "    0 & 1 & 0 \\\\\n",
    "    0 & 2 & 1  \\\\\n",
    "    1 & -1 & 5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Inversely, this also implies that these pivot columns can also describe the nonpivot columns through a series of linear combinations eliminations. In other words, the original entries for the pivot columns form a _basis_ for the remaining columns. This is important kind of identification because we've effectively illiminated space (as in literal bits) without any loss of information.\n",
    "\n",
    "We can say that matrix $A$ has a _rank_ of 3, because the dimension of the column space (or basis) is 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection\n",
    "\n",
    "An intuitive way of reframing the question of \"what is the projection of $a$ onto $b$?\" is \"what the shadow of $a$ cast onto $b$, if the angle of the light were to be perpendicular of b?\".\n",
    "\n",
    "<img src=\"../../assets/0-foundations/projection.png\" alt=\"projection\" style=\"width: 300px;\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To project vector $a$ onto $b$ we perform $proj_ba=\\frac{ab}{|b|^2}b$. To understand how this is mathematically derived, lets compose the image above into facts.\n",
    "\n",
    "* $a_1 = proj_b(a)$\n",
    "* $a_1 = c \\cdot b$, the projection is some scaler of b\n",
    "* $a_2 = proj_b(a) - a$\n",
    "* $a_1 \\cdot a_2 = 0$, since these two vectors are orthogonal.\n",
    "\n",
    "Taking these facts, we can derive the formula for projection. The goal is to ultimately find $c$, since the projection is $cb$ when $b$ is given.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    a_2 \\cdot b &= 0 \\\\\n",
    "    (a - cb) \\cdot b &= 0 \\\\\n",
    "    b \\cdot a - cb \\cdot b &= 0 \\\\\n",
    "    a \\cdot b &= cb \\cdot b \\\\\n",
    "    c &= \\frac{a \\cdot b}{b \\cdot b}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore since $a_1 = cb$, the $proj_ba=\\frac{ab}{|b|^2}b$. We can actually show that if $b$ is a unit vector, then the porject, $proj_ba$ simplified to dot product between the two vectors, $a \\cdot b$. If you let $b=1$ and plug this into the original function, you can quickly see that.\n",
    "\n",
    "In practice,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
