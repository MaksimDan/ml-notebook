{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "A random forest is a model ensemble of decision trees. Meaning it is not fundamentally different that a decision tree. It just has more bells and whistles. They were created in order to help alleviate some of the issues lone decision trees tend to deal with like high variance or overfitting.\n",
    "\n",
    "Now as a model ensemble, random forests employ the bagging technique, and more specifically random patching. Meaning that multiple decisions trees are trained a random subset of features per the node splitting process. This mean every decision tree will:\n",
    "\n",
    "1. Randomly subset of the training data.\n",
    "2. Randomly select a subset of the features.\n",
    "3. Random select a subset of the features per split.\n",
    "\n",
    "As a consequence, each tree is trained on a different set of features generating more diversity in the process, hence overcomming the overfitting problem. In addition, this also forcing the trees to be uncorrelated. Correlated trees would otherwise lead to the same votes almost redundently. In the end, a random forest would average out the predictions of all the trees, or use any other statistical model like frequency in the case of classifiers.\n",
    "\n",
    "Since each classifier can be trained independently, it's also pretty easy to see how you can train each each classifier in parallel using GPU or CPU Cores.\n",
    "\n",
    "You can approximate a random forest using this sklearn's `BaggingClassifier`.\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16), \n",
    "                            n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "### Click-Through Prediction with a Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# read in data\n",
    "DATA_DIR = join('..', '..', '..', 'data', 'click-rate-prediction')\n",
    "click_df = pd.read_csv(join(DATA_DIR, 'train.csv'), nrows=150000)\n",
    "click_df.drop(['id', 'hour', 'device_id', 'device_ip'], axis=1, inplace=True)\n",
    "\n",
    "# encode\n",
    "click_df = click_df.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "# split X and y into np matricies explicitly\n",
    "col_names = list(click_df)\n",
    "X_names, y_names = list(filter(lambda name: name != 'click', col_names)), ['click']\n",
    "X, y = np.array(click_df[X_names]), np.array(click_df[y_names])\n",
    "\n",
    "# one hot encoding for categorical distance constaint\n",
    "X_train = OneHotEncoder(categories='auto').fit_transform(X)\n",
    "\n",
    "# split X and y into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   26.3s\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   33.0s\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   46.2s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   58.9s\n",
      "[Parallel(n_jobs=4)]: Done  43 tasks      | elapsed:   58.9s\n",
      "[Parallel(n_jobs=4)]: Done  44 tasks      | elapsed:   58.9s\n",
      "[Parallel(n_jobs=4)]: Done  45 tasks      | elapsed:   58.9s\n",
      "[Parallel(n_jobs=4)]: Done  46 tasks      | elapsed:   58.9s\n",
      "[Parallel(n_jobs=4)]: Done  47 tasks      | elapsed:   58.9s\n",
      "[Parallel(n_jobs=4)]: Done  48 tasks      | elapsed:   58.9s\n",
      "[Parallel(n_jobs=4)]: Done  49 tasks      | elapsed:   58.9s\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-eb35c09fb8f9>\", line 16, in <module>\n",
      "    gsearch.fit(X_train, y_train)\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\", line 722, in fit\n",
      "    self._run_search(evaluate_candidates)\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\", line 1191, in _run_search\n",
      "    evaluate_candidates(ParameterGrid(self.param_grid))\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\", line 711, in evaluate_candidates\n",
      "    cv.split(X, y, groups)))\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 930, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 833, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 521, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/concurrent/futures/_base.py\", line 427, in result\n",
      "    self._condition.wait(timeout)\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/threading.py\", line 295, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2018, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/posixpath.py\", line 388, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/posixpath.py\", line 422, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/Users/danielm/anaconda3/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# grid search for parameter optimization\n",
    "# n_estimators = number of trees in the forest used for majority voting\n",
    "parameters = {'max_depth': np.arange(5, 30, 3),\n",
    "              'n_estimators': np.arange(100, 120, 5),\n",
    "              'min_samples_split': np.arange(20, 60, 15)}\n",
    "\n",
    "# another parameter worth considering: max_features: which specifies the number of\n",
    "# random features to consider upon splitting\n",
    "# verbose for progressbar (the quantity specifies the detail)\n",
    "rforest = RandomForestClassifier(criterion='entropy')\n",
    "gsearch = GridSearchCV(rforest, parameters, n_jobs=4, cv=3, scoring='roc_auc', verbose=10)\n",
    "gsearch.fit(X_train, y_train)\n",
    "print(gsearch.best_params_)\n",
    "\n",
    "rforest_best = gsearch.best_estimator_\n",
    "rforest_prob_pred = rforest_best.predict_proba(X_test)[:, 1]\n",
    "rforest_auc = roc_auc_score(y_test, rforest_prob_pred)\n",
    "print(f'The ROC AUC on testing set is using optimized rforest classifier is {rforest_auc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
