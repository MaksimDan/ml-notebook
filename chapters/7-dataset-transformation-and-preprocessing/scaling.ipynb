{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling\n",
    "\n",
    "Some machine learning algorithms are sensative to data with features with largely different scales. This is only natural in many cases because features deal with different units, in different context, at different times. For example, _SGD_-based linear regression and SVR models both require features to be standardized, and so scaling features is appropriate here. In general, any feature that requires any form of distances across the dimensional plane together should be scaled.\n",
    "\n",
    "A better question might be, when is scaling not important? In general Naive Bayes or tree based algorithms are not sensative to features across different scale because these algorithms do not look at two or more features together at the same time. In general however, it is a good idea to scale the data you are working with as part of your preprocessing step.\n",
    "\n",
    "\n",
    "## Standard Scaling\n",
    "\n",
    "With standard scaling, features are transformed to be on a compariable scale. This is done so by removing the mean and dividing my the standard diviation.\n",
    "\n",
    "$$x^{(i)}_{scaled} = \\frac{x^{(i)} - \\bar{x}}{\\sigma(x)}$$\n",
    "\n",
    "Here, $x$ is the feature vector column i, $\\bar{x}$ is the mean, and $\\sigma(x)$ is its respective standard deviation. First, subtracting the mean centralizes a measurable different from the mean. From this point forward, we look at the distance a unit is from its mean value. Then dividing by the standard deviation results is the total number of standard deviations a particular unit is from its mean. As a result, if the distribution is was normal, we obtain a normal distribution around 0 and a variance of 1, $N(0,1)$.\n",
    "\n",
    "### When is Standard Scaling Nessessary?\n",
    "\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might otherwise behave badly if the individual features do not more or less look like standard normally distributed data. For example, RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models).\n",
    "\n",
    "Visually, we can check to see if any the numerical attributes of our data require any form of standard scaling bin each column: `df.hist(bins=n_bins, figsize=fig_size)`.\n",
    "\n",
    "\n",
    "**How does having a nonstandardized distribution make it more difficult for some machine learning algorithms to detect patterns?**\n",
    "\n",
    "Standardization is much less effected by outliers. For example, a min max scaler would truncate all values by the maximum (an outlier) quite heavily, were standardization would be a much more resilient against this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09366299, 0.03691003, 0.29804019, 0.99082518, 0.84544584],\n",
       "       [0.45723152, 0.82265235, 0.46982767, 0.87457698, 0.76771025],\n",
       "       [0.94679804, 0.18674826, 0.82732283, 0.01595558, 0.30078996],\n",
       "       [0.80262036, 0.19260009, 0.28741615, 0.99084315, 0.82630977],\n",
       "       [0.69315085, 0.30691736, 0.10966073, 0.68344271, 0.40839732]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "X = np.random.rand(5, 5)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.68954013, -1.00577693, -0.41350968,  0.76566388,  0.94342782],\n",
       "       [-0.4732482 ,  1.89694216,  0.29392418,  0.44743657,  0.60345216],\n",
       "       [ 1.16456077, -0.45223884,  1.76611633, -1.90302357, -1.43861795],\n",
       "       [ 0.6822249 , -0.4306208 , -0.45726026,  0.76571307,  0.85973645],\n",
       "       [ 0.31600266, -0.00830559, -1.18927057, -0.07578994, -0.96799848]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Scaling (Normalization)\n",
    "\n",
    "Perhaps the simplest transformation that could be made is a Min-Max scaler. A Min-Max scaler binds a value to a specific range (by default to 0-1, but this case be changed to be any range via `feature_range`). Some Neural networks, for example, require the output to be between 0 and 1.\n",
    "\n",
    "$$x^{(i)}_{scaled} = \\frac{x^{(i)(j)} - min(x^{(i)})}{max(x^{(i)}) - min(x^{(i)})}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
