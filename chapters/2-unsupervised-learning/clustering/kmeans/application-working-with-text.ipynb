{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application - Working with Text\n",
    "\n",
    "Goal: Classify each document to the appropriate newsgroup using clustering using a bag of words type model.\n",
    "\n",
    "We expect certain words to capture the semantic properties of a document. Word frequency was a way of quantifying this. For example, a document relating to python might contain words like 'class', 'def', and 'init' and documents such as snakes might contains words like 'afraid', 'slither', or 'bite'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import names\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\n",
    "def lemmatize_and_clean(doc):\n",
    "    \"\"\"\n",
    "    obj: tokenize, filter, and lemmatize document\n",
    "    \"\"\"\n",
    "    names_set = set(names.words())\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    f1 = [word.lower() for word in nltk.tokenize.word_tokenize(doc)]\n",
    "    f2 = [word for word in f1 if word.isalpha() and word not in names_set]\n",
    "    return ' '.join(lemmatizer.lemmatize(word) for word in f2)\n",
    "\n",
    "\n",
    "# fetch and load the data\n",
    "dataset = fetch_20newsgroups(subset='all', shuffle=True, random_state=42)\n",
    "labels = dataset.target\n",
    "\n",
    "# we happen to know the true labels in advance\n",
    "true_num_clusters = np.unique(labels).shape[0]\n",
    "\n",
    "# data cleaning and preprocessing\n",
    "dataset = [lemmatize_and_clean(post) for post in dataset.data]\n",
    "\n",
    "# transform text into numerical features and run dimensionality reduction\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "svd = TruncatedSVD()\n",
    "normalizer = Normalizer(copy=False)\n",
    "pl = make_pipeline(vectorizer, svd, normalizer)\n",
    "X = pl.fit_transform(dataset)\n",
    "\n",
    "km = KMeans(n_clusters=true_num_clusters)\n",
    "km.fit(X)\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Score: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
