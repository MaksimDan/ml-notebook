{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " -- need to revamp, didnt take a look at this yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization (NMF) for Clustering\n",
    "\n",
    "* Non-negative Matrix Factorization is a topic modeling algorithm that factorizes one matrix as a product of two smaller matrices such that all three matrices have no negative values. This can be though of decomposing a whole into two parts.\n",
    "* Recall the rules for matrix multiplication:\n",
    "    * $(a \\text{ by } b) \\text{ x } (c \\text{ by } d) = (a \\text{ by } d)$\n",
    "\n",
    "$$W*H = V$$\n",
    "\n",
    "\n",
    "### Fact\n",
    "* This algorithm does not have a perfect solution but is numerically approximated. \n",
    "* $W$ and $H$ are non-unique.\n",
    "* Typically produces sparse matrices\n",
    "\n",
    "\n",
    "### In Practice\n",
    "* There are some papers that suggest NMF is equivalent to k-means. This is true to some degree, but it is more accurate to say that it behaves like k-means.\n",
    "* Works particularly well with documents. \n",
    "\n",
    "\n",
    "### Other Uses\n",
    "* Dimensionality reduction (similar to PCA)\n",
    "\n",
    "\n",
    "**What happens when we decompose a term-document matrix in two?**\n",
    "* Suppose all the documents news groups (like above). We could imagine commonly co-occuring words grouped together in the same vector, and then each article would have a certain weight of the topic of the group. So for example suppose we have a food vector, then each document would have a particular weighting of words like 'tasty' or 'sushi'. \n",
    "* This is where the term topic modeling comes from because we are forming topic vectors that decompose from a term-matrix.\n",
    "* For example suppose we had the following document-term frequency.\n",
    "\n",
    "|          | labour | energy | market     | employment | \n",
    "|----------|--------|--------|------------|----| \n",
    "| Speech 1 | 36     | 3      | 45         | 54 | \n",
    "| Speech 2 | 4      | 34     | 23         | 31 | \n",
    "| Speech 3 | 9      | 65     | 11         | 0  | \n",
    "| Speech 4 | 17     | 3      | 3          | 0  | \n",
    "| Speech 5 | 0      | 14     | 7          | 4  | \n",
    "\n",
    "\n",
    "* And our algorithm decomposed it to the following $W$ and $H$ matrices, using `n_components=2`.\n",
    "\n",
    "\n",
    "* $W$ or weights matrix\n",
    "\n",
    "|          | Factor 1    | Factor 2    | \n",
    "|----------|-------------|-------------| \n",
    "| Speech 1 | 0.021135218 | 0.63411542  | \n",
    "| Speech 2 | 0.26893587  | 0.24248544  | \n",
    "| Speech 3 | 0.56521061  | 2.2204e-16  | \n",
    "| Speech 4 | 0.028056074 | 0.088332775 | \n",
    "| Speech 5 | 0.11666223  | 0.035066365 | \n",
    "\n",
    "\n",
    "* $H$ or factors matrix\n",
    "\n",
    "|          | labour    | energy     | market     | employment |            | \n",
    "|----------|-----------|------------|------------|------------| \n",
    "| Factor 1 | 10.975128 | 118.16503  | 21.246259  | 2.2204e-16 | \n",
    "| Factor 2 | 55.024872 | 0.83496782 | 67.753741  | 89         | \n",
    "\n",
    "\n",
    "* Interpretations\n",
    "    * (5 x 2) * (2 x 4) = (5 x 4)\n",
    "    * Factor Matrix\n",
    "        * The factors are the two components.\n",
    "        * The values are the weights belonging to the particular factor. For example, energy and market seem to be the two largest weighted features in relation to factor 1. In general, the co-occurring features will have weights in relation to one another.\n",
    "    * Weight Matrix\n",
    "        * We can use this matrix to determine what component or factor each speech belongs to by considering the maximum value. For example, speech 1 is more so related to factor 2 than it is to factor 1. So can also observe to what degree this difference is.\n",
    "        \n",
    "\n",
    "\n",
    "**Some Parameters for sklearn's NMF**\n",
    "* `n_components`: the number of topics (or clusters)\n",
    "* `alpha`: multiplication factor for regularization terms (parameter tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  max, ma, air, end, usa, distribution, university, organization\n",
      "2:  wa, did, said, people, know, say, armenian, went\n",
      "3:  db, bit, data, left, right, time, stuff, place\n",
      "4:  widget, window, application, use, value, set, display, work\n",
      "5:  file, gun, control, state, house, law, crime, article\n",
      "6:  space, center, year, data, nasa, research, ha, program\n",
      "7:  entry, file, program, section, rule, use, number, source\n",
      "8:  team, hockey, game, league, new, season, wa, player\n",
      "9:  drive, disk, hard, support, card, scsi, head, speed\n",
      "10:  image, format, file, color, data, display, software, program\n",
      "11:  god, jesus, atheist, christian, doe, believe, people, religion\n",
      "12:  president, ha, think, going, know, package, wa, said\n",
      "13:  line, organization, subject, writes, article, university, just, like\n",
      "14:  use, ground, doe, subject, ha, need, used, power\n",
      "15:  output, file, program, line, return, entry, write, open\n",
      "16:  key, encryption, chip, law, technology, government, clipper, device\n",
      "17:  turkish, jew, armenian, wa, people, war, government, book\n",
      "18:  available, version, server, ftp, sun, subject, source, contact\n",
      "19:  internet, user, posting, email, information, computer, mail, service\n",
      "20:  mac, ibm, pc, color, window, machine, hardware, program\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "\n",
    "nmf = NMF(n_components=20, random_state=43).fit(transformed)\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(f'{topic_idx+1}: ', ', '.join([cv.get_feature_names()[i] for i in topic.argsort()[:-9:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Above we observe the commonly used terms within each topic. For example observe how the terms in topic 11 all refer to religious terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
