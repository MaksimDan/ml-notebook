{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics\n",
    "\n",
    "Performance metric or loss functions measure the overall performance of a model. They give an idea how much error the system typically makes in its predictions\n",
    "\n",
    "## Terminologies\n",
    "\n",
    "### Error vs Accuracy\n",
    "\n",
    "Error and accuracy describe the same thing in different ways. Error is ratio of incorrect evaluations and accuracy is the ratio of correct evaluations. $\\text{Error} = 1-\\text{Accuracy}$, $\\text{Accuracy} = 1-\\text{Error}$\n",
    "\n",
    "### Base Line Accuracies\n",
    "\n",
    "For your model to have some significance, it has to do better than baseline accuracy, which is $\\frac{1}{k}$ if $k$ is the number of possible classfications. The lowest goal, but definition is just to do better than random guessing.\n",
    "\n",
    "Another baseline accuracy in case of classification is just estimating based on the proportion of classifications. For example, if 90% of classifications are \"A\", then by default a classifier that returns all \"A\", will bias an accuracy of 90%. For this reason accuracy is not a perferred metric for skewed datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Regression\n",
    "\n",
    "### MSE - Root Mean Square Error\n",
    "\n",
    "$$\\text{MSE}(X, h) = \\frac{1}{m} \\sum_{i=1}^m(h(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "\n",
    "Mean squared error, or the squared loss of over the expected value. This loss function computes the average distance a models predictions are off from the true value. This base lose function forms the basis for many other loss functions.\n",
    "\n",
    "**Why it's Good**\n",
    "\n",
    "Evaluations or predictions that are very away from the true more are \"punished\" than points that are closer to the truth due to the squared term. In this way, the model learns to avoid making really bad predictions on the trade off of making many smaller errors. The final value is also averaged, so it can be interpret directly inline with the context prediction, e.g. \"A average error of $45,000\".\n",
    "\n",
    "\n",
    "### RMSE - Root Mean Square Error\n",
    "\n",
    "$$\\text{RMSE}(X, h) = \\sqrt{\\frac{1}{m} \\sum_{i=1}^m(h(x^{(i)}) - y^{(i)})^2}$$\n",
    "\n",
    "Same as above, but it takes the squared root on top of  $\\sqrt{x^2}=x$  in order to obtain back the original scale of the data.\n",
    "\n",
    "\n",
    "### MAE - Mean Absolute Loss\n",
    "\n",
    "$$\\text{MAE}(X, h) = \\frac{1}{m} \\sum_{i=1}^m|h(x^{(i)}) - y^{(i)}|$$\n",
    "\n",
    "Mean absolute loss has a few advantages.\n",
    "\n",
    "1. It provides a clear metric how close the predictions are to the actual value.\n",
    "2. Points further away are punished at the same linear scale as points closer away. This can be useful in some contexts. For example, outliers that produce errors do not make the error grow out of control, as they would have in the squared context because it is squared.\n",
    "\n",
    "\n",
    "### $R^2$ - R-squared\n",
    "\n",
    "R-squared is a value between 0 and 1 that indicate the goodness of fit for a model. On this scale, 1 indicates a perfect fit and 0 indicates no fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
