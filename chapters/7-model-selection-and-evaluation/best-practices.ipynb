{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices\n",
    "\n",
    "1. When evaluating your model, always work with the training data. I built a habit of always evaluating with the test set even when I was in the process of optimizing the model. The reason why the testing set should always remain untouched is because this is what is most closely inlined with the real world. The real world will always have new data streams thrown at the model that it cannot exactly reproduce exactly (most of the time) from the training set. One question you may ask is, well if I am training and evaluating from the training set, arent I just optimizing and overfitting my model for the training set? Yes. The alternative with evalating on the test set would be the same. But in the case that evaluation took place on the test set would mean that performance would be over optimistic when real world data comes in. In summary, evaluate your model when all optimization specs are in with the training data.\n",
    "\n",
    "2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
