{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Updating\n",
    "\n",
    "As new data comes in, it should go through the same processing pipelines as done previously (scaling, feature engineering, feature selection, dimensionality reduction, and so on). The challenge comes when how we chose to update our models.\n",
    "\n",
    "There are several different approaches we can make use of our models. Each approach is different depending on the circumstances.\n",
    "\n",
    "\n",
    "## Static Models\n",
    "\n",
    "The procedure looks something like this:\n",
    "\n",
    "1. Build your model with all training data.\n",
    "2. Make predictions using this model\n",
    "\n",
    "This is useful in situations where we would expect the data to change all that much. For example, hand digit recognition, or face recognition are all, for the most part, static models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([223.3151919 ,  94.35900362, 190.45246179, 162.88419784,\n",
       "       212.92811312, 136.30183464, 117.17933268,  88.67517379,\n",
       "       135.47915109, 221.68399609, 189.48544713, 163.85041462,\n",
       "       148.00984505, 116.37226253, 201.66061882, 106.02869693,\n",
       "       264.60286421,  93.16352761, 118.2312028 , 131.07663212,\n",
       "       213.37358002,  75.78037125, 106.2207279 , 106.3541514 ,\n",
       "        67.46927024, 209.90449269, 111.84487039, 101.77849948,\n",
       "       190.24802448, 118.55492724])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# can alternatively use pickle, but joblib is more optimized\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "# suppose that our pretrained model started like this\n",
    "init = 30\n",
    "dataset = datasets.load_diabetes()\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "# data initialization\n",
    "X_train = X[:-init, :]\n",
    "y_train = y[:-init]\n",
    "\n",
    "# data preprocessing - [save state]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "# fit the model with scaled data - [save state]\n",
    "X_scaled_train = scaler.transform(X_train)\n",
    "regressor = SVR(C=20)\n",
    "regressor.fit(X_scaled_train, y_train)\n",
    "joblib.dump(regressor, \"regressor.pkl\")\n",
    "\n",
    "# mocking new data that comes in\n",
    "X_train = X[-init:, :]\n",
    "y_train = y[-init:]\n",
    "\n",
    "# load the previous models, (if not loaded)\n",
    "my_scaler = joblib.load(\"scaler.pkl\")\n",
    "my_regressor = joblib.load(\"regressor.pkl\")\n",
    "\n",
    "# predict\n",
    "X_train = my_scaler.transform(X_train)\n",
    "predictions = my_regressor.predict(X_train)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Dynamic Models\n",
    "\n",
    "Depending on thinks like the efficiency of the training algorithm, the rate of data inflow, the size of the data, or the expected response time for a prediction, another possiblity is to completely update the model from scratch every time, or with batches of new data. This is feasable if the data is small and does not scale, or the model algorithm is efficient enough to retrain in a practical time frame.\n",
    "\n",
    "The steps here are:\n",
    "\n",
    "1. Train your model.\n",
    "2. Make predictions.\n",
    "3. Retrain your model from scratch when a new batch of information comes in.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Learning/Updating\n",
    "\n",
    "In the third case, the model is updated as batches of new information comes in, just like with _Small Dynamic Models_. The primary difference is that in this case, rather than having to completely retain from scratch, a _partial fit_ technique is employed. This option is not available to all models, (or at least not discovered yet), but when available, this option is always preferrable to _Small Dynamic Models_.\n",
    "\n",
    "Some examples that employ _partial fit_ include:\n",
    "* `IncrementalPCA`\n",
    "\n",
    "\n",
    "Alternatively, numpys `memmap` classes allows manipulation of a large array stored on disk, and read it as if it were entirely stored in memory.\n",
    "\n",
    "```python`\n",
    "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n",
    "\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo ---\n",
    "\n",
    "- Sklearn implements online learning with `partial_fit` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
