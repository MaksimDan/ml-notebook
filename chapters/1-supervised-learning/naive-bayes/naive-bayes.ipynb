{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "* Naive Bayes falls under the category of probabilistic classifiers, that computes the probability of a each feature belonging in each class in order to make a prediction.\n",
    "    * Naive: because it goes along with the assumption that the features are mutually independent (do not affect one another).\n",
    "    * Bayes: Follows Bays Theorem.\n",
    "    \n",
    "    \n",
    "## Bays Theorem\n",
    "\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)}$$\n",
    "\n",
    "* Intuition: Defines the probability of A occurring in the probability subset of B.\n",
    "    * If A on its own is high, then naturally P(A|B) is also high, but if B is high, there is less room for A to be high so A becomes lower.\n",
    "* A good way to get familiar with naive ways is by doing some probability examples.\n",
    "\n",
    "### Warm Up Examples\n",
    "\n",
    "#### Example 1\n",
    " \n",
    "> A box has 3 white balls and 7 black balls. Three balls are randomly selected one by one without replacement. \n",
    ">    1. if the first two balls are black, what is the probability the third ball will be black? \n",
    ">    2. If the first two balls are the same color. what is the probability the third ball will be black? \n",
    "\n",
    "![mc_ex1](../../../assets/naive_bayes/markov_chain_ex1.png)\n",
    "\n",
    "* A great way to visualize a probabilistic process is by using _Markov Chains_. A Markov chain is a model of some random process that happens over time. The Markov property states that whatever happens next in a process only depends on it's current state. It doesn't have a \"memory\" of how it was before.\n",
    "\n",
    "#### Example 2\n",
    "\n",
    "> Suppose that I% of the population have a disease called stataphobia. There is a diagnostic test with 0.95 probability of being positive when a person has stataphobia and 0.8 probability of being negative when a person does not have stataphobia. If a person tests positive for stataphobia, what is the probability they have stataphobia? \n",
    "\n",
    "![mc_ex2](../../../assets/naive_bayes/markov_chain_ex2.png)\n",
    "\n",
    "\n",
    "### Naive Bayes Examples\n",
    "\n",
    "#### Example 1\n",
    "\n",
    "Given the following training data, using a naive Bayes classifier, predict y of new sample {x1 = S, x2 = C, x3 = H, x4 = S}.\n",
    "\n",
    "\n",
    "| x1 | x2 | x3 | x4 | y | \n",
    "|----|----|----|----|---| \n",
    "| S  | H  | H  | W  | N | \n",
    "| S  | H  | H  | S  | N | \n",
    "| O  | H  | H  | W  | Y | \n",
    "| R  | M  | H  | W  | Y | \n",
    "| R  | C  | N  | W  | Y | \n",
    "| R  | C  | N  | S  | N | \n",
    "| O  | C  | N  | S  | Y | \n",
    "| S  | M  | H  | W  | N | \n",
    "| S  | C  | N  | W  | Y | \n",
    "| R  | M  | N  | W  | Y | \n",
    "| S  | M  | N  | S  | Y | \n",
    "| O  | M  | H  | S  | Y | \n",
    "| O  | H  | N  | W  | Y | \n",
    "| R  | M  | H  | S  | N | \n",
    "\n",
    "\n",
    "![nb_sol1](../../../assets/naive_bayes/naive_bayes_sol1.png)\n",
    "\n",
    "1. Our goal here is to find the classification that is maximized given previous training examples. The first equation summarizes this. Find the argument (y=c* which can be either Y or N) that is maximized given the state {x1 = S, x2 = C, x3 = H, x4 = S}. \n",
    "2. Using Bayes rule, the condition is reversed and we can observe that becomes maximized.\n",
    "3. The final line shows why the process is naive. We are simply counting the ratio of occurrences each column contains the desired configuration conditionally on the classification. If there are fewer occurrences then naturally the probability decreases. \n",
    "\n",
    "We can write this in general as:\n",
    "\n",
    "$$P(y_k|X) \\propto P(x|y_k)P(y_k) = P(x_1|y_k)*P(x_2|y_k)* \\dots * P(x_n|y_k)*P(y_k)$$\n",
    "\n",
    "\n",
    "#### Example 2\n",
    "\n",
    "Given the following training data...\n",
    "\n",
    "| ID | Terms in email              | Is spam | \n",
    "|----|-----------------------------|---------| \n",
    "| 1  | Click win prize             | 1       | \n",
    "| 2  | Click meeting setup meeting | 0       | \n",
    "| 3  | Prize free pizza            | 1       | \n",
    "| 4  | Click prize free            | 1       | \n",
    "\n",
    "Predict the following test sample:\n",
    "\n",
    "| ID | Terms in email              | Is spam | \n",
    "|----|-----------------------------|---------| \n",
    "| 5  | Free setup meeting free     | ?       | \n",
    "\n",
    "\n",
    "\n",
    "$P(S|x) = P(S)*P(\\text{free}|S)*P(\\text{setup}|S)*P(\\text{meeting}|S) = \\frac{3}{4}*\\frac{2+1}{9+7}*\\frac{0+1}{9+7}*\\frac{0+1}{9+7} = 0.0006667$\n",
    "\n",
    "$P(NS|x) = P(NS)*P(\\text{free}|NS)*P(\\text{setup}|NS)*P(\\text{meeting}|NS) = \\frac{1}{4}*\\frac{0+1}{4+7}*\\frac{1+1}{4+7}*\\frac{2+1}{4+7} = 0.0015$\n",
    "\n",
    "Conclusion: more likely to be not spam.\n",
    "\n",
    "**Explanation**\n",
    "* 1/4 of the documents are spam, therefore the remaining 3/4 are not.\n",
    "* Every conditional probability looks ratio of the term frequency for a respective class. For example, there are 9 terms in the spam class and two of them contain the word 'free'. On the other hand there are 4 terms in the not spam class, and none of them contain the word free.\n",
    "    * The is where the 1/7 comes in. In order to avoid zeroing out the other term frequencies (which potentially could be very high!), we apply _Laplace smoothing_. Within Laplace smoothing, all terms begin there count at 1, rather than zero, hence the +1 that we see. To compensate, we divide by the number of unique terms in the denominator (there are 7 of these terms).\n",
    "\n",
    "\n",
    "## Implementing Naive Bayes\n",
    "\n",
    "We will be building a spam classifier using data downloaded from: http://www.aueb.gr/users/ion/data/enron-spam/preprocessed/enron1.tar.gz\n",
    "\n",
    "* In this dataset, there are approximately 3,672 ham (legitimate) emails and 1,500 spam emails. So there are approximately 2 non spam email examples per spam examples.\n",
    "\n",
    "### Data Exploration\n",
    "#### Sample Ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: mcmullen gas for 11 / 99\n",
      "jackie ,\n",
      "since the inlet to 3 river plant is shut in on 10 / 19 / 99 ( the last day of\n",
      "flow ) :\n",
      "at what meter is the mcmullen gas being diverted to ?\n",
      "at what meter is hpl buying the residue gas ? ( this is the gas from teco ,\n",
      "vastar , vintage , tejones , and swift )\n",
      "i still see active deals at meter 3405 in path manager for teco , vastar ,\n",
      "vintage , tejones , and swift\n",
      "i also see gas scheduled in pops at meter 3404 and 3405 .\n",
      "please advice . we need to resolve this as soon as possible so settlement\n",
      "can send out payments .\n",
      "thanks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATA_PATH = os.path.join('data', 'enron1')\n",
    "\n",
    "\n",
    "file_path = os.path.join(DATA_PATH, 'ham', '0007.1999-12-14.farmer.ham.txt')\n",
    "with open(file_path, 'r') as infile:\n",
    "    ham_sample = infile.read()\n",
    "print(ham_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: stacey automated system generating 8 k per week parallelogram\n",
      "people are\n",
      "getting rich using this system ! now it ' s your\n",
      "turn !\n",
      "we ' ve\n",
      "cracked the code and will show you . . . .\n",
      "this is the\n",
      "only system that does everything for you , so you can make\n",
      "money\n",
      ". . . . . . . .\n",
      "because your\n",
      "success is . . . completely automated !\n",
      "let me show\n",
      "you how !\n",
      "click\n",
      "here\n",
      "to opt out click here % random _ text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(DATA_PATH, 'spam', '0058.2003-12-21.GP.spam.txt')\n",
    "with open(file_path, 'r') as infile:\n",
    "    spam_sample = infile.read()\n",
    "print(spam_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in the data for spam emails\n",
    "emails, labels = [], []\n",
    "SPAM_DIR = os.path.join(DATA_PATH, 'spam')\n",
    "spam_files = [os.path.join(SPAM_DIR, spam) for spam in os.listdir(SPAM_DIR) if spam.endswith('.txt')]\n",
    "for spam_file in spam_files:\n",
    "    with open(spam_file, 'r', encoding = \"ISO-8859-1\") as infile:\n",
    "        emails.append(infile.read())\n",
    "        labels.append(1)\n",
    "\n",
    "# loading in the data for ham emails\n",
    "HAM_DIR = os.path.join(DATA_PATH, 'ham')\n",
    "ham_files = [os.path.join(HAM_DIR, ham) for ham in os.listdir(HAM_DIR) if ham.endswith('.txt')]\n",
    "for ham_file in ham_files:\n",
    "    with open(ham_file, 'r', encoding = \"ISO-8859-1\") as infile:\n",
    "        emails.append(infile.read())\n",
    "        labels.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subject dobmeos with hgh my energy level ha gone up stukm introducing doctor formulated hgh human growth hormone also called hgh is referred to in medical science a the master hormone it is very plentiful when we are young but near the age of twenty one our body begin to produce le of it by the time we are forty nearly everyone is deficient in hgh and at eighty our production ha normally diminished at least advantage of hgh increased muscle strength loss in body fat increased bone density lower blood pressure quickens wound healing reduces cellulite improved vision wrinkle disappearance increased skin thickness texture increased energy level improved sleep and emotional stability improved memory and mental alertness increased sexual potency resistance to common illness strengthened heart muscle controlled cholesterol controlled mood swing new hair growth and color restore read more at this website unsubscribe'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data preprocess the data by cleaning it. this will include:\n",
    "# - number and punctuation removal\n",
    "# - human name removal (optional)\n",
    "# - stop words removal\n",
    "# - lemmatization\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import names\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "NAMES = set(names.words())\n",
    "\n",
    "def my_filter(doc):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    f1 = [word.lower() for word in word_tokenize(doc)]\n",
    "    f2 = [word for word in f1 if word.isalpha() and word not in NAMES]\n",
    "    return ' '.join(lemmatizer.lemmatize(word) for word in f2)\n",
    "\n",
    "\n",
    "emails = [my_filter(email) for email in emails]\n",
    "emails[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be using the term-frequency as our features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "cv = CountVectorizer(stop_words=\"english\", max_features=500)\n",
    "term_docs = cv.fit_transform(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'access', 'account', 'accounting', 'act', 'action', 'activity', 'actual', 'actuals', 'add']\n"
     ]
    }
   ],
   "source": [
    "# these are our features (most frequent terms) in order\n",
    "print(cv.get_feature_names()[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# and we can access a row in our feature matrix by indexing by a document ID\n",
    "# .A converts the document from sparse to non.sparse\n",
    "print(term_docs[1].A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice how the length of each row matches with the length of the number of features\n",
    "assert(len(cv.get_feature_names()) == len(term_docs[1].A[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 418,\n",
       " 'energy': 125,\n",
       " 'ha': 178,\n",
       " 'called': 47,\n",
       " 'young': 497,\n",
       " 'le': 231,\n",
       " 'time': 446,\n",
       " 'production': 345,\n",
       " 'loss': 250,\n",
       " 'swing': 425,\n",
       " 'new': 285,\n",
       " 'color': 69,\n",
       " 'read': 357,\n",
       " 'website': 481,\n",
       " 'prescription': 337,\n",
       " 'low': 252,\n",
       " 'cost': 86,\n",
       " 'online': 301,\n",
       " 'order': 306,\n",
       " 'direct': 111,\n",
       " 'click': 66,\n",
       " 'thanks': 439,\n",
       " 'list': 241,\n",
       " 'people': 319,\n",
       " 'change': 58,\n",
       " 'able': 0,\n",
       " 'partner': 310,\n",
       " 'team': 431,\n",
       " 'investment': 212,\n",
       " 'account': 2,\n",
       " 'agreement': 17,\n",
       " 'project': 348,\n",
       " 'based': 34,\n",
       " 'contact': 77,\n",
       " 'set': 394,\n",
       " 'business': 44,\n",
       " 'request': 368,\n",
       " 'act': 4,\n",
       " 'opportunity': 304,\n",
       " 'come': 71,\n",
       " 'send': 389,\n",
       " 'million': 272,\n",
       " 'state': 411,\n",
       " 'dollar': 114,\n",
       " 'area': 29,\n",
       " 'cover': 90,\n",
       " 'regard': 362,\n",
       " 'better': 37,\n",
       " 'special': 405,\n",
       " 'following': 152,\n",
       " 'tax': 427,\n",
       " 'share': 395,\n",
       " 'provide': 349,\n",
       " 'process': 342,\n",
       " 'money': 275,\n",
       " 'long': 247,\n",
       " 'form': 154,\n",
       " 'need': 282,\n",
       " 'security': 387,\n",
       " 'john': 221,\n",
       " 'number': 295,\n",
       " 'america': 23,\n",
       " 'stock': 415,\n",
       " 'company': 73,\n",
       " 'release': 364,\n",
       " 'sale': 379,\n",
       " 'international': 210,\n",
       " 'world': 490,\n",
       " 'market': 259,\n",
       " 'corp': 83,\n",
       " 'st': 409,\n",
       " 'location': 245,\n",
       " 'year': 496,\n",
       " 'event': 133,\n",
       " 'plan': 329,\n",
       " 'demand': 107,\n",
       " 'center': 57,\n",
       " 'make': 254,\n",
       " 'notice': 293,\n",
       " 'report': 367,\n",
       " 'information': 209,\n",
       " 'provided': 350,\n",
       " 'service': 393,\n",
       " 'statement': 412,\n",
       " 'offer': 297,\n",
       " 'buy': 45,\n",
       " 'sell': 388,\n",
       " 'use': 465,\n",
       " 'person': 322,\n",
       " 'complete': 74,\n",
       " 'party': 311,\n",
       " 'forward': 155,\n",
       " 'looking': 249,\n",
       " 'risk': 375,\n",
       " 'actual': 7,\n",
       " 'result': 370,\n",
       " 'including': 204,\n",
       " 'stop': 416,\n",
       " 'email': 121,\n",
       " 'web': 480,\n",
       " 'com': 70,\n",
       " 'additional': 10,\n",
       " 'work': 488,\n",
       " 'technology': 432,\n",
       " 'today': 447,\n",
       " 'doc': 112,\n",
       " 'wa': 477,\n",
       " 'like': 236,\n",
       " 'want': 478,\n",
       " 'th': 437,\n",
       " 'size': 399,\n",
       " 'day': 102,\n",
       " 'charge': 60,\n",
       " 'xl': 493,\n",
       " 'good': 173,\n",
       " 'morning': 277,\n",
       " 'free': 158,\n",
       " 'best': 36,\n",
       " 'mail': 253,\n",
       " 'real': 358,\n",
       " 'product': 344,\n",
       " 'office': 298,\n",
       " 'right': 374,\n",
       " 'hour': 187,\n",
       " 'pay': 315,\n",
       " 'available': 33,\n",
       " 'currently': 94,\n",
       " 'item': 215,\n",
       " 'doe': 113,\n",
       " 'pill': 325,\n",
       " 'help': 181,\n",
       " 'visit': 475,\n",
       " 'site': 398,\n",
       " 'let': 233,\n",
       " 'solution': 402,\n",
       " 'limited': 237,\n",
       " 'add': 9,\n",
       " 'sure': 422,\n",
       " 'purchase': 352,\n",
       " 'check': 61,\n",
       " 'week': 483,\n",
       " 'getting': 170,\n",
       " 'using': 468,\n",
       " 've': 473,\n",
       " 'viagra': 474,\n",
       " 'price': 338,\n",
       " 'place': 328,\n",
       " 'http': 197,\n",
       " 'www': 491,\n",
       " 'link': 239,\n",
       " 'html': 196,\n",
       " 'january': 220,\n",
       " 'thank': 438,\n",
       " 'software': 401,\n",
       " 'try': 456,\n",
       " 'start': 410,\n",
       " 'action': 5,\n",
       " 'cd': 54,\n",
       " 'don': 115,\n",
       " 'receive': 359,\n",
       " 'end': 124,\n",
       " 'gary': 164,\n",
       " 'used': 466,\n",
       " 'deal': 103,\n",
       " 'record': 361,\n",
       " 'march': 257,\n",
       " 'access': 1,\n",
       " 'month': 276,\n",
       " 'customer': 95,\n",
       " 'save': 380,\n",
       " 'professional': 346,\n",
       " 'valid': 470,\n",
       " 'internet': 211,\n",
       " 'info': 208,\n",
       " 'payment': 316,\n",
       " 'remove': 365,\n",
       " 'case': 52,\n",
       " 'reply': 366,\n",
       " 'message': 265,\n",
       " 'operation': 303,\n",
       " 'ce': 55,\n",
       " 'effective': 120,\n",
       " 'source': 404,\n",
       " 'content': 78,\n",
       " 'type': 461,\n",
       " 'window': 487,\n",
       " 'microsoft': 269,\n",
       " 'image': 201,\n",
       " 'nd': 281,\n",
       " 'align': 19,\n",
       " 'border': 40,\n",
       " 'tr': 451,\n",
       " 'td': 429,\n",
       " 'src': 408,\n",
       " 'width': 486,\n",
       " 'height': 180,\n",
       " 'believe': 35,\n",
       " 'just': 226,\n",
       " 'look': 248,\n",
       " 'href': 194,\n",
       " 'index': 206,\n",
       " 'nbsp': 280,\n",
       " 'font': 153,\n",
       " 'family': 138,\n",
       " 'pt': 351,\n",
       " 'going': 172,\n",
       " 'love': 251,\n",
       " 'face': 135,\n",
       " 'newsletter': 287,\n",
       " 'address': 11,\n",
       " 'br': 41,\n",
       " 'david': 101,\n",
       " 'desk': 108,\n",
       " 'note': 292,\n",
       " 'include': 203,\n",
       " 'phone': 324,\n",
       " 'great': 176,\n",
       " 'news': 286,\n",
       " 'activity': 6,\n",
       " 'll': 243,\n",
       " 'home': 184,\n",
       " 'line': 238,\n",
       " 'hope': 185,\n",
       " 'know': 229,\n",
       " 'date': 100,\n",
       " 'book': 39,\n",
       " 'thing': 440,\n",
       " 'term': 435,\n",
       " 'prior': 340,\n",
       " 'canada': 50,\n",
       " 'transport': 455,\n",
       " 'possible': 335,\n",
       " 'original': 307,\n",
       " 'fax': 140,\n",
       " 'xp': 494,\n",
       " 'way': 479,\n",
       " 'letter': 234,\n",
       " 'advise': 14,\n",
       " 'daily': 96,\n",
       " 'feel': 144,\n",
       " 'cialis': 63,\n",
       " 'na': 278,\n",
       " 'current': 93,\n",
       " 'high': 183,\n",
       " 'city': 64,\n",
       " 'create': 91,\n",
       " 'page': 308,\n",
       " 'north': 291,\n",
       " 'dec': 104,\n",
       " 'june': 225,\n",
       " 'entered': 128,\n",
       " 'mike': 271,\n",
       " 'corporation': 84,\n",
       " 'fee': 143,\n",
       " 'development': 109,\n",
       " 'rate': 355,\n",
       " 'received': 360,\n",
       " 'steve': 414,\n",
       " 'past': 312,\n",
       " 'option': 305,\n",
       " 'unit': 463,\n",
       " 'field': 145,\n",
       " 'regarding': 363,\n",
       " 'data': 99,\n",
       " 'global': 171,\n",
       " 'future': 161,\n",
       " 'net': 284,\n",
       " 'brian': 43,\n",
       " 'oil': 299,\n",
       " 'power': 336,\n",
       " 'friday': 159,\n",
       " 'fact': 137,\n",
       " 'robert': 377,\n",
       " 'pm': 331,\n",
       " 'exchange': 134,\n",
       " 'program': 347,\n",
       " 'control': 81,\n",
       " 'hi': 182,\n",
       " 'did': 110,\n",
       " 'quality': 353,\n",
       " 'got': 174,\n",
       " 'position': 334,\n",
       " 'adobe': 12,\n",
       " 'suite': 419,\n",
       " 'update': 464,\n",
       " 'sent': 390,\n",
       " 'confirm': 76,\n",
       " 'jan': 219,\n",
       " 'life': 235,\n",
       " 'increase': 205,\n",
       " 'question': 354,\n",
       " 'delivery': 106,\n",
       " 'communication': 72,\n",
       " 'error': 131,\n",
       " 'farmer': 139,\n",
       " 'firm': 148,\n",
       " 'problem': 341,\n",
       " 'point': 332,\n",
       " 'transaction': 454,\n",
       " 'advice': 13,\n",
       " 'tuesday': 458,\n",
       " 'mark': 258,\n",
       " 'resource': 369,\n",
       " 'follow': 151,\n",
       " 'say': 381,\n",
       " 'thought': 442,\n",
       " 'estimate': 132,\n",
       " 'contract': 80,\n",
       " 'management': 255,\n",
       " 'support': 421,\n",
       " 'facility': 136,\n",
       " 'industry': 207,\n",
       " 'section': 386,\n",
       " 'supply': 420,\n",
       " 'fw': 162,\n",
       " 'survey': 423,\n",
       " 'review': 371,\n",
       " 'december': 105,\n",
       " 'financial': 147,\n",
       " 'changed': 59,\n",
       " 'little': 242,\n",
       " 'manager': 256,\n",
       " 'needed': 283,\n",
       " 'user': 467,\n",
       " 'paid': 309,\n",
       " 'trading': 453,\n",
       " 'id': 198,\n",
       " 'producer': 343,\n",
       " 'said': 378,\n",
       " 'gas': 165,\n",
       " 'issue': 214,\n",
       " 'schedule': 382,\n",
       " 'performance': 320,\n",
       " 'group': 177,\n",
       " 'revised': 372,\n",
       " 'operating': 302,\n",
       " 'thursday': 444,\n",
       " 'trade': 452,\n",
       " 'wednesday': 482,\n",
       " 'cc': 53,\n",
       " 'server': 392,\n",
       " 'feb': 141,\n",
       " 'natural': 279,\n",
       " 'file': 146,\n",
       " 'copy': 82,\n",
       " 'te': 430,\n",
       " 'flow': 149,\n",
       " 'february': 142,\n",
       " 'think': 441,\n",
       " 'hank': 179,\n",
       " 'september': 391,\n",
       " 'old': 300,\n",
       " 'path': 314,\n",
       " 'application': 27,\n",
       " 'availability': 32,\n",
       " 'ect': 118,\n",
       " 'mary': 261,\n",
       " 'august': 31,\n",
       " 'ray': 356,\n",
       " 'george': 169,\n",
       " 'working': 489,\n",
       " 'utility': 469,\n",
       " 'brenda': 42,\n",
       " 'stephanie': 413,\n",
       " 'zero': 498,\n",
       " 'houston': 188,\n",
       " 'nom': 288,\n",
       " 'total': 450,\n",
       " 'texas': 436,\n",
       " 'james': 218,\n",
       " 'agree': 16,\n",
       " 'pg': 323,\n",
       " 'jones': 222,\n",
       " 'cotton': 88,\n",
       " 'zone': 499,\n",
       " 'soon': 403,\n",
       " 'tomorrow': 449,\n",
       " 'duke': 116,\n",
       " 'attached': 30,\n",
       " 'volume': 476,\n",
       " 'employee': 122,\n",
       " 'pop': 333,\n",
       " 'july': 224,\n",
       " 'created': 92,\n",
       " 'scheduled': 383,\n",
       " 'pipeline': 327,\n",
       " 'spot': 406,\n",
       " 'correct': 85,\n",
       " 'tx': 459,\n",
       " 'aol': 26,\n",
       " 'plant': 330,\n",
       " 'meeting': 262,\n",
       " 'monday': 274,\n",
       " 'computron': 75,\n",
       " 'marketing': 260,\n",
       " 'continue': 79,\n",
       " 'ticket': 445,\n",
       " 'april': 28,\n",
       " 'ee': 119,\n",
       " 'allocation': 22,\n",
       " 'meyers': 267,\n",
       " 'scott': 385,\n",
       " 'bob': 38,\n",
       " 'iv': 216,\n",
       " 'thu': 443,\n",
       " 'storage': 417,\n",
       " 'october': 296,\n",
       " 'meter': 266,\n",
       " 'logistics': 246,\n",
       " 'yahoo': 495,\n",
       " 'period': 321,\n",
       " 'smith': 400,\n",
       " 'camp': 49,\n",
       " 'graf': 175,\n",
       " 'pipe': 326,\n",
       " 'fuel': 160,\n",
       " 'accounting': 3,\n",
       " 'allen': 20,\n",
       " 'ami': 24,\n",
       " 'pricing': 339,\n",
       " 'taylor': 428,\n",
       " 'jackie': 217,\n",
       " 'coastal': 68,\n",
       " 'katherine': 227,\n",
       " 'fred': 157,\n",
       " 'november': 294,\n",
       " 'michael': 268,\n",
       " 'invoice': 213,\n",
       " 'megan': 263,\n",
       " 'unify': 462,\n",
       " 'lee': 232,\n",
       " 'imbalance': 202,\n",
       " 'julie': 223,\n",
       " 'howard': 189,\n",
       " 'lloyd': 244,\n",
       " 'im': 200,\n",
       " 'gc': 167,\n",
       " 'tom': 448,\n",
       " 'gathering': 166,\n",
       " 'vance': 472,\n",
       " 'forwarded': 156,\n",
       " 'carlos': 51,\n",
       " 'tap': 426,\n",
       " 'hou': 186,\n",
       " 'valley': 471,\n",
       " 'lisa': 240,\n",
       " 'nomination': 289,\n",
       " 'tu': 457,\n",
       " 'susan': 424,\n",
       " 'weissman': 484,\n",
       " 'daren': 97,\n",
       " 'melissa': 264,\n",
       " 'enron': 126,\n",
       " 'darren': 98,\n",
       " 'calpine': 48,\n",
       " 'fyi': 163,\n",
       " 'sherlyn': 396,\n",
       " 'anita': 25,\n",
       " 'allocated': 21,\n",
       " 'hpl': 190,\n",
       " 'teco': 433,\n",
       " 'buyback': 46,\n",
       " 'clem': 65,\n",
       " 'counterparty': 89,\n",
       " 'sitara': 397,\n",
       " 'hplc': 191,\n",
       " 'entex': 129,\n",
       " 'rita': 376,\n",
       " 'wynne': 492,\n",
       " 'chokshi': 62,\n",
       " 'scheduling': 384,\n",
       " 'mmbtu': 273,\n",
       " 'aimee': 18,\n",
       " 'lannou': 230,\n",
       " 'midcon': 270,\n",
       " 'noms': 290,\n",
       " 'wellhead': 485,\n",
       " 'hsc': 195,\n",
       " 'revision': 373,\n",
       " 'equistar': 130,\n",
       " 'pat': 313,\n",
       " 'txu': 460,\n",
       " 'actuals': 8,\n",
       " 'clynes': 67,\n",
       " 'gco': 168,\n",
       " 'ena': 123,\n",
       " 'spreadsheet': 407,\n",
       " 'iferc': 199,\n",
       " 'katy': 228,\n",
       " 'flowed': 150,\n",
       " 'eastrans': 117,\n",
       " 'cec': 56,\n",
       " 'pec': 317,\n",
       " 'pefs': 318,\n",
       " 'hplo': 193,\n",
       " 'cotten': 87,\n",
       " 'tenaska': 434,\n",
       " 'aep': 15,\n",
       " 'hplno': 192,\n",
       " 'enronxgate': 127}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in order map each feature to its cooresponding column, we can access the following\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the labels by its type\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def build_label_index(labels):\n",
    "    label_index = defaultdict(list)\n",
    "    for i, label in enumerate(labels):\n",
    "        label_index[label].append(i)\n",
    "    return label_index\n",
    "\n",
    "label_index = build_label_index(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the prior function\n",
    "\n",
    "def get_prior(label_index):\n",
    "    \"\"\"\n",
    "    objective: compute prior probabilities, which finds the probability\n",
    "    of a class occuring\n",
    "    :param label_index: dict(int:list) - class to index (location) mapping\n",
    "    :return: dict(int:float) - class to proir probability \n",
    "    \"\"\"\n",
    "    total_count = sum(len(indices) for _, indices in label_index.items())\n",
    "    return {label:len(indices)/total_count for label, indices in label_index.items()}\n",
    "\n",
    "prior = get_prior(label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the likelihood function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_likelihood(term_doc_matrix, label_index, smoothing=0):\n",
    "    \"\"\"\n",
    "    objective: compute likelihood probability, which is the probability of \n",
    "    a particular sequences occuring conditionally on a class\n",
    "    \n",
    "    | class | f1       | f2       | . | . | . | fn       | \n",
    "    |-------|----------|----------|---|---|---|----------| \n",
    "    | c1    | P(f1|c1) | P(f2|c1) |   |   |   | P(fn|c1) | \n",
    "    | c2    | P(f1|c2) | P(f2|c2) |   |   |   | P(fn|c2) | \n",
    "    | .     |          |          |   |   |   |          | \n",
    "    | .     |          |          |   |   |   |          | \n",
    "    | .     |          |          |   |   |   |          | \n",
    "    | cn    | P(f1|cn) | P(f2|cn) |   |   |   | P(fn|cn) | \n",
    "\n",
    "    :param term_doc_matrix: sparse np matrix - as determined by some vectorizer\n",
    "    :param label_index: dict(int:list) - class to index (location) mapping\n",
    "    :smoothing: int - integer to start counting from\n",
    "    :return: dict(int:float) - class to likelihood probability product(P(feature|class))\n",
    "    \"\"\"\n",
    "    likelihood = {}\n",
    "    for label, indices in label_index.items():\n",
    "        # index [each row] in the term matrix for a particular class. this returns a new submatrix\n",
    "        # then and sum each column (axis = 0), which denote occurances per feature\n",
    "        # finally, add + smoothing to each column to ensure non-zero multiplication (if applicable)\n",
    "        likelihood[label] = np.asarray(term_doc_matrix[indices, :].sum(axis=0) + smoothing)[0]\n",
    "        \n",
    "        # compute the total count for the denominater, and perform element wise division to compute a \n",
    "        # a single row of likelihoods\n",
    "        total_count = likelihood[label].sum()\n",
    "        likelihood[label] = likelihood[label] / float(total_count)\n",
    "    return likelihood\n",
    "\n",
    "likelihood = get_likelihood(term_docs, label_index, smoothing=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.06413167e-03 9.38618703e-04 8.62219506e-04 8.29476993e-04\n",
      " 9.82275386e-05]\n",
      "[0.00105635 0.00137524 0.00442469 0.00051821 0.00408586]\n",
      "['able', 'access', 'account', 'accounting', 'act']\n"
     ]
    }
   ],
   "source": [
    "# confirm that there 500 likelihoods (number of features specified)\n",
    "assert(len(likelihood[0]) == 500)\n",
    "\n",
    "# sample first 5 likelihoods for ham and spam class respectively\n",
    "print(likelihood[0][0:5])\n",
    "print(likelihood[1][0:5])\n",
    "\n",
    "# which coorespond to the following terms\n",
    "print(cv.get_feature_names()[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject dobmeos with hgh my energy level ha gone up stukm introducing doctor formulated hgh human growth hormone also called hgh is referred to in medical science a the master hormone it is very plentiful when we are young but near the age of twenty one our body begin to produce le of it by the time we are forty nearly everyone is deficient in hgh and at eighty our production ha normally diminished at least advantage of hgh increased muscle strength loss in body fat increased bone density lower blood pressure quickens wound healing reduces cellulite improved vision wrinkle disappearance increased skin thickness texture increased energy level improved sleep and emotional stability improved memory and mental alertness increased sexual potency resistance to common illness strengthened heart muscle controlled cholesterol controlled mood swing new hair growth and color restore read more at this website unsubscribe\n",
      "\n",
      "subject re tenaska iv i tried calling you this am but your phone rolled to someone else s voicemail can you call me when you get a chance original message from farmer daren j sent thursday january pm to hill garrick subject re tenaska iv rick i ve had a couple of meeting today i m sorry i m just getting back to you i tried to call but the voice mail said that you were unavailable so give me a call when you get a chance d original message from hill garrick sent wednesday january pm to farmer daren j subject re tenaska iv i ll call you on thursday what s a good time original message from farmer daren j sent wednesday january pm to hill garrick cc olsen michael subject tenaska iv rick we need to talk about the ability of ena to continue it the current role a agent of tenaska iv since the end on november ena ha not been able to complete gas trading transaction we can not find any counterparties to trade physical gas in texas this of course is due to the bankruptcy a a result we are not able to sale tenaska s excess fuel we did contact brazos to ask if they would buy a portion of the gas at a gas daily price but they do not want it gas daily pricing ha been below the firm contract price for a while in december we had to cut day from the th through the th for january we haven t had to cut yet but i am sure that the pipe will ask u to do this in the near future for november activity which wa settled in dec ena owes tenaska iv for the excess supply that we sold however due to the bankruptcy we could not make payment out ena could not pay the supplier or the pipeline james armstrong paid the counterparties directly i think that he should continue to do this for dec and jan we should not transfer any fund from tenaska iv to ena i don t know how enron s ownership in the plant factor out in the bankruptcy preceding but we need to determine how to go forward with the fuel management please give me a call or e mail me we can get together sometime thurs or fri morning d\n",
      "\n",
      "[{1: 0.9981313687075513, 0: 0.0018686312924487828}, {1: 2.2592734561111858e-23, 0: 1.0}]\n"
     ]
    }
   ],
   "source": [
    "# build posterior function\n",
    "# - the posterior = likelihood * prior (when I mean '=' here I am implying proportionality)\n",
    "# - this might cause an overflow issue because the likelihood consists of  hundreds of values <= 1\n",
    "#   - to combat this we take the log likelihood, and later convert it back to obtain the original probability\n",
    "#     - posterior = prior * likelihood\n",
    "#     - posterior = exp(log(prior * likelihood))\n",
    "#     - posterior = exp(log(prior * likelihood))\n",
    "#     - posterior = exp(log(prior) + log(likelihood))\n",
    "# \n",
    "\n",
    "def get_posterior(term_doc_matrix, prior, likelihood):\n",
    "    \"\"\"\n",
    "    objective: computes the posterior based on prior and likelihood\n",
    "    :param term_doc_matrix: sparse matrix - vectorized term frequencies by doc\n",
    "    :param prior: dict(int, float) - mapping from class to prior probability\n",
    "    :param likelihood: dict(int, np.array(float)) - mapping from class to \n",
    "    conditional likelihood probabilitites\n",
    "    :return: [dict(int, float)] - posterior probabilities for each class per document\n",
    "    \"\"\"\n",
    "    n_docs, posteriors = term_doc_matrix.shape[0], []\n",
    "    labels = prior.keys()\n",
    "    for i in range(n_docs):\n",
    "        # look into what features the current document has\n",
    "        # and only consider non-zero counts\n",
    "        cur_doc = term_doc_matrix.getrow(i)\n",
    "        non_zero_indices, non_zero_counts = cur_doc.indices, cur_doc.data\n",
    "        posterior = {label:np.exp(np.log(prior[label]) \n",
    "                         + np.sum(np.log(likelihood[label][non_zero_indices]))) \n",
    "                     for label in labels}\n",
    "        \n",
    "        # now normalize the probabilities so that they sum to 1\n",
    "        sum_posterior = sum(posterior.values())\n",
    "        posteriors.append({label:_posterior/sum_posterior for label, _posterior in posterior.items()}) \n",
    "    return posteriors\n",
    "\n",
    "\n",
    "# test with a ham and spam email respectively\n",
    "sample_test = [emails[0], emails[-1]]\n",
    "print('\\n\\n'.join(sample_test), end='\\n\\n')\n",
    "\n",
    "term_docs_test = cv.transform(sample_test)\n",
    "posterior = get_posterior(term_docs_test, prior, likelihood)\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Classifier Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on 1707 testing samples is with threshold=0.5 is 0.92.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(emails, labels, test_size=0.33, random_state=42)\n",
    "term_docs_train = cv.fit_transform(X_train)\n",
    "label_index = build_label_index(y_train)\n",
    "\n",
    "# the main observation here is that we compute our prior and likelihood using the training data\n",
    "prior = get_prior(label_index)\n",
    "likelihood = get_likelihood(term_docs_train, label_index, smoothing=1)\n",
    "\n",
    "# and use this to test data it has not seen before to test its performance\n",
    "term_docs_test = cv.transform(X_test)\n",
    "posterior = get_posterior(term_docs_test, prior, likelihood)\n",
    "\n",
    "def get_accuracy(posterior, true_labels, t=.5):\n",
    "    \"\"\"\n",
    "    objective: classify samples as either ham or spam on the\n",
    "    basis of a threshold against the posterior\n",
    "    :param true_labels: int - 1 for spam, 0 for ham\n",
    "    :param threshold: float - decision criterion\n",
    "    :return: float - ratio of correct:total classifications \n",
    "    \"\"\"\n",
    "    # here there are two particular ways we can be correct\n",
    "    # 1) the email is actually spam, and we classified it with a probability > .5\n",
    "    # 2) the email is actually not spam, and we classified it with a probability > .5\n",
    "    return np.mean([1 if (true==1 and pred[1] >= t) or (true==0 and pred[0] > t) else 0 \n",
    "                    for pred, true in zip(posterior, true_labels)])\n",
    "\n",
    "\n",
    "accuracy = get_accuracy(posterior, y_test, t=.5)\n",
    "print(f'The accuracy on {len(y_test)} testing samples is with threshold={.5} is {accuracy:.2f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage via Sci-kit Learn API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 1.80494500e-10],\n",
       "       [1.00000000e+00, 6.93842036e-75],\n",
       "       [6.43054246e-01, 3.56945754e-01],\n",
       "       [1.00000000e+00, 1.26282643e-12],\n",
       "       [1.00000000e+00, 3.69207533e-12],\n",
       "       [1.53290848e-04, 9.99846709e-01],\n",
       "       [0.00000000e+00, 1.00000000e+00],\n",
       "       [1.00000000e+00, 4.21663711e-19],\n",
       "       [1.00000000e+00, 1.75639432e-13],\n",
       "       [3.10923660e-01, 6.89076340e-01]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "    \n",
    "\n",
    "# alpha is also known as the smoothing parameter\n",
    "# fit prior is a boolean indicating whether the prior should be fit on the training set\n",
    "clf = MultinomialNB(alpha=1.0, fit_prior=True)\n",
    "\n",
    "# train the classifier X and y, data and output\n",
    "clf.fit(term_docs_train, y_train)\n",
    "\n",
    "# nowing compute the posterior on the testing set with the prior and likelihood from training set\n",
    "# note that the default theshold is .5\n",
    "prediction_prob = clf.predict_proba(term_docs_test)\n",
    "prediction_prob[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can directly obtain the classifications with predict method\n",
    "# if the probability is > .5 thna class 1 is assigned, otherwise class 0 is assigned\n",
    "prediction = clf.predict(term_docs_test)\n",
    "prediction[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on 1707 testing samples using MultinomialNB with threshold=0.5 is 0.92.\n"
     ]
    }
   ],
   "source": [
    "# and finally, we can measure accuracy of the classifier using the score method\n",
    "accuracy = clf.score(term_docs_test, y_test)\n",
    "print(f'The accuracy on {len(y_test)} testing samples using MultinomialNB with threshold={.5} is {accuracy:.2f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1098,   93],\n",
       "       [  44,  472]], dtype=int64)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# provided the true labels, predicted labels, and labels where order \n",
    "# does not matter to the extend of swapping what true and false is\n",
    "confusion_matrix(y_test, prediction, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.84, Recall: 0.91, F1-score: 0.87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# precision, recall, and f1-score\n",
    "ps = precision_score(y_test, prediction, pos_label=1)\n",
    "rs = recall_score(y_test, prediction, pos_label=1)\n",
    "f1s = f1_score(y_test, prediction, pos_label=1)\n",
    "\n",
    "print(f'Precision: {ps:.2f}, Recall: {rs:.2f}, F1-score: {f1s:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.92      0.94      1191\n",
      "          1       0.84      0.91      0.87       516\n",
      "\n",
      "avg / total       0.92      0.92      0.92      1707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# all three metrics in one, also in dependences on what label is which\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9588711199630302"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# area under the roc curve\n",
    "pos_prob = prediction_prob[:, 1]\n",
    "roc_auc_score(y_test, pos_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning for Optimal Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# as an evaluation metric, we are going to use cross validation in conjunction with\n",
    "# the auc under roc as the performance metric\n",
    "k_fold = StratifiedKFold(n_splits=10)\n",
    "emails_np, labels_np = np.array(emails), np.array(labels)\n",
    "\n",
    "# before training and testing our classiifer we subjectively chose between the following options:\n",
    "# - smoothing (offset counting parameter)\n",
    "# - number of features (selected 500)\n",
    "# - whether or not to use the prior as part of the mutlplier for NB\n",
    "max_features_option = [2000, 4000, 8000]\n",
    "smoothing_factor_option = [0.5, 1.0, 1.5, 2.0]\n",
    "fit_prior_option = [True, False]\n",
    "all_options = [max_features_option, smoothing_factor_option, fit_prior_option]\n",
    "auc_k_fold_record = []\n",
    "\n",
    "\n",
    "# iterate through all the combinations of model features and record the results\n",
    "for features_opt, smoothing_opt, prior_opt in list(itertools.product(*all_options)):\n",
    "    # k-fold cross validation as our performance metric\n",
    "    auc_temp = []\n",
    "    for train_indices, test_indices in k_fold.split(emails_np, labels_np):\n",
    "        # split\n",
    "        X_train, X_test = emails_np[train_indices], emails_np[test_indices]\n",
    "        y_train, y_test = labels_np[train_indices], labels_np[test_indices]\n",
    "        \n",
    "        # vectorize\n",
    "        cv = CountVectorizer(stop_words=\"english\", max_features=features_opt)\n",
    "        term_docs_train = cv.fit_transform(X_train)\n",
    "        term_docs_test = cv.transform(X_test)\n",
    "        \n",
    "        # train\n",
    "        clf = MultinomialNB(alpha=smoothing_opt, fit_prior=prior_opt)\n",
    "        clf.fit(term_docs_train, y_train)\n",
    "        \n",
    "        # test\n",
    "        prediction_prob = clf.predict_proba(term_docs_test)\n",
    "        pos_prob = prediction_prob[:, 1]\n",
    "        auc = roc_auc_score(y_test, pos_prob)\n",
    "        auc_temp.append(auc)\n",
    "        \n",
    "    auc_k_fold_record.append({'features_opt': features_opt, 'smoothing_opt': smoothing_opt, \n",
    "                              'prior_opt': prior_opt, 'k_fold_auc': np.mean(auc_temp)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "features_opt         8000\n",
       "k_fold_auc       0.985681\n",
       "prior_opt            True\n",
       "smoothing_opt         0.5\n",
       "Name: 16, dtype: object"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results of the combination run\n",
    "auc_records = pd.DataFrame(auc_k_fold_record)\n",
    "max_auc_row = auc_records.iloc[auc_records['k_fold_auc'].idxmax()]\n",
    "max_auc_row"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
