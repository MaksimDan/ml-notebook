{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Classification\n",
    "\n",
    "With multilabel classification we have two or more classifications for a single set of attributes. As a real life example, we could have a face detector that was trained to recognize  Jane, Alice and Bob. If it comes across than say (Alice, Bob) or (Alice, Bob, Dan) It would output a [0, 1, 1] in both case in recognition that Alice and Bob are present in the photo.\n",
    "\n",
    "\n",
    "**Example**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os.path import join\n",
    "from myutils.config import config\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "\n",
    "\n",
    "data_root = join(config['data_dir'], 'digits v2')\n",
    "X_train, y_train = loadlocal_mnist(images_path=join(data_root, 'train-images.idx3-ubyte'), \n",
    "                                   labels_path=join(data_root, 'train-labels.idx1-ubyte'))\n",
    "X_test, y_test = loadlocal_mnist(images_path=join(data_root, 't10k-images.idx3-ubyte'), \n",
    "                                 labels_path=join(data_root, 't10k-labels.idx1-ubyte'))\n",
    "\n",
    "\n",
    "shuffle_index_train = np.random.permutation(X_train.shape[0])\n",
    "shuffle_index_test = np.random.permutation(X_test.shape[0])\n",
    "\n",
    "X_train, y_train = X_train[shuffle_index_train], y_train[shuffle_index_train]\n",
    "X_test, y_test = X_test[shuffle_index_test], y_test[shuffle_index_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, ill create two different toy binary classifications. These two classification will be horizontally stacked to perform a multilabeled final class. We'll then train a k-nearest-neighbor classifier to learn a multilabeled classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "y_train_7_or_more = (y_train >= 7)\n",
    "y_train_odd = (y_train % 2 == 1)\n",
    "y_train_multilabel = np.c_[y_train_7_or_more, y_train_odd]\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_train_multilabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: [[ True  True]]\n",
      "actual    : 9\n"
     ]
    }
   ],
   "source": [
    "# sample prediction: large but not odd\n",
    "print(f'prediction: {knn_clf.predict([X_test[123]])}')\n",
    "print(f'actual    : {y_test[123]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does this work?**\n",
    "\n",
    "MY GUESS\n",
    "\n",
    "If we can already understand how a single classification process works, then in the case of a multilabeled classifier can just generalize by individually training a classifier for each column of `y_train_multilabel`. Then process of `predict` would be to reference each classifier iteratively.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "One option is compute the average cross validated f1-score. Recall that the f1-score is a weighted aggregate of precision and recall on the classification. In our case, because this is a multilabeled classification, an f1-score would be computed per each `y_train_multilabel`, and the average would be computed.\n",
    "\n",
    "Another option would be perform a weighted f1-score. For example, we could have more pictures of Alice than of Bob. This would mean that classifier would \"learn\" Alice better, and thus have a more accurate score of Alice than of Bob. In this way, it would make sense then to weight the classifier for Alice proportional to the frequency of Alice photos, and to the frequency of Bob photos. The score in this case wouldn't become skewed due to having barely trained on Bob photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "\n",
    "# y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_train_multilabel, cv=3)\n",
    "# f1_score(y_train, y_train_knn_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multioutput Classification\n",
    "\n",
    "As a generalization of the above, lets us focus on the case where the output of a classifier is to reproduce a multi-class for a set of classes. As a real world example, suppose that we now wanted to detect a specific set of people in a photo. For each person, a classifier could identify person A, B, C... for every person in a photograph.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Suppose we wanted to remove noise from an image. In this case, we have 28x28 pixeled image, where each pixel has a class of 255 color intensity/shades of black. The goal would be to taken noisey image X and output a clear image y which is a multilabeled classfication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# input X = noisey images, train and test sets\n",
    "noise_matrix = np.random.randint(0, 100, (len(X_train), 784))\n",
    "X_train_mod = X_train + noise_matrix\n",
    "noise_matrix = np.random.randint(0, 100, (len(X_test), 784))\n",
    "X_test_mod = X_test + noise_matrix\n",
    "\n",
    "# output y clear images, train and test sets\n",
    "y_train_mod = X_train\n",
    "y_test_mod = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fd773a118d86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplot_digit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_digit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-fd773a118d86>\u001b[0m in \u001b[0;36mplot_digit\u001b[0;34m(digit_matrix)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_digit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigit_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdigit_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdigit_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigit_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# train a multilabled model\n",
    "knn_clf.fit(X_train_mod, y_train_mod)\n",
    "clean_digit = knn_clf.predict([X_test_mod[1]])\n",
    "\n",
    "def plot_digit(digit_matrix):\n",
    "    digit_matrix = digit_matrix.reshape(28, 28)\n",
    "    plt.imshow(digit_matrix, cmap=matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "plot_digit(clean_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
