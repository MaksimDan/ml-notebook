{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Hand Digit Recognition\n",
    "\n",
    "Notes and code were adapted from: http://neuralnetworksanddeeplearning.com\n",
    "\n",
    "## Logical Operations\n",
    "\n",
    "We can show the ANN's are capabile of running logical operations.\n",
    "\n",
    "<img src=\"../../../../assets/1-supervised-learning/ann-logical-ops.png\" style=\"width: 750px;\"/>\n",
    "\n",
    "If we make the assumption that nueron C is only true when there are two or more incoming signals that are true, then ...\n",
    "1. C=A is the identify function because when A fires or does not fire, C will do the same.\n",
    "2. Since C requires both A and B to be active, so if either is of, it will not fire.\n",
    "3. A can fire, or B can fire, or both can fire independently. The result would be C firing since A and B both stem two signals each.\n",
    "4. C will fire only if A and B does not fire. Suppose A fires and B fires. Then we have 2-1 = 1 total signals since B degates a signal. Suppose A fires and B does not fires. Then we 3 total total signals, resulting in C firing. \n",
    "\n",
    "This is important because it tells us that any kind of gate can be built with a perceptron, including NAND gates, which are universal for computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Foundations\n",
    "\n",
    "### TLU - Threshold Logical Unit\n",
    "\n",
    "The neuron itself can be thought of as a number (between 0 and 1 if it a sigmoid neuron) that comes from the sum of the previous neurons multiplied connected weight plus some bias. It is structured in such in order to describe how important an input is. The TLU provides this foundation for neural networks.\n",
    "\n",
    "The TLU takes in a series of variables and weights as numbers and outputs an either on or off value. Mathematically, the TLU will compute a weighted sum of its inputs ($z = w_1x_1 + w_2x_2 + \\dots + w_nx_n = x^Tw$), and then applies a step function on top of that sum to produce a 1 or 0.\n",
    "\n",
    "$$h_w(x) = step(z), \\text{ where } z = x^Tw$$\n",
    "\n",
    "<img src=\"../../../../assets/1-supervised-learning/tlu.png\" style=\"width: 750px; margin:auto 0;display: block;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical step function will activate the signal if the weights of its inputs are >=0. Look at this unit a bit more closely, it behaves identically to a linear logistic classifier or a linear regressor. That is, it takes a vecotr row or input x, followed by a respected weight vector, and then thresholds the weighted sum of this inputs to produce a binary classification: zero or one.\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\\n",
    "      1 & \\mbox{if } \\sum_j w_j x_j > \\mbox{ threshold}\n",
    "      \\end{array} \\right.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11cb0ccf8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from mltoolbox.draw.math import heaviside\n",
    "\n",
    "heaviside()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, a single TLU can be used to classify iris flowers based off petal length. The training process here would be to identify the  optimal weight vector that minimizes the overall error in our training samples.\n",
    "\n",
    "We can rewrite the single binary output of the function, $h_w(x) = step(x^Tw)$ equivalently as.\n",
    "\n",
    "\\begin{equation}\n",
    "   [1|0]\n",
    "    =\n",
    "    \\text{step} \\left(\n",
    "    \\begin{bmatrix}\n",
    "        x_{11} x_{12} x_{13} \\dots x_{1n}\n",
    "    \\end{bmatrix}\n",
    "    %\n",
    "    \\begin{bmatrix}\n",
    "        w_{11} \\\\ \n",
    "        w_{21} \\\\ \n",
    "        w_{31} \\\\ \n",
    "        \\dots \\\\ \n",
    "        w_{n1}\n",
    "    \\end{bmatrix}\n",
    "    \\right)\n",
    "\\end{equation}\n",
    "\n",
    "* $x$ is a single row input, with $n$ different dimensions.\n",
    "* $w$ is an learned weight vector that is also $n$ dimensional to respectfully match each feature in $x$\n",
    "* The output is a a number 1, 0 single it was filtered through the step function.\n",
    "\n",
    "For example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def step(x):\n",
    "    return int(x >= 0)\n",
    "step(np.array([1,2,3])@np.array([1,2,3]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Activation Function\n",
    "\n",
    "Similiar to the step function, there is a more commonly used activation function that outputs a real number rather than a binary, \"fire or not\" kind of signal.\n",
    "\n",
    "A neural network learns in the process of small changes to weights, that incrementally reduce the the output error. However, some small changes can cause a cascading effect on other artificial neurons, including output neurons to flip from 0 to 1. This is where the sigmoid neuron comes in. Sigmoid neurons translate our output that is 0 and 1 to a real number between 0 and 1. Rather than getting a discrete value between 0 and 1, now we can infer the notable degree of change. So now whereas a perceptron that does not fire will have no effect, the equivalent sigmoid neuron will have an unnotiably small effect ($\\approx 0$). In effect, the sigmoid activiation function is a smoothed version of the perceptron step function.\n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\sigma(z) = \\frac{1}{1+e^{-z}} = \\frac{1}{1+\\exp(-\\sum_j w_j x_j-b)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "In summary, the smoothness of the sigmoid functions allows us how small changes in $w$ will always have small changes in the output. According to the partial derivative, these changes are linear to $w$ and the bias $b$. More onto this later, but this means that during the training process, the weights are able to get nudged in the direction most meaningful to them.\n",
    "\n",
    "There are other alternatives to the sigmoid function, such as relu for example - which is often used in deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11cbc7860>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mltoolbox.draw.math import sigmoid\n",
    "\n",
    "sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "The perceptron is a just a single layer of TLU's, where each input is connected to each TLU. When this kind of thing happens, its called a fully connected layer. That fully connected layer in our case would be the layer of TLU's.\n",
    "\n",
    "<img src=\"../../../../assets/1-supervised-learning/perceptron.png\" style=\"width: 750px; margin:auto 0;display: block;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input neurons in our cases are provided as illustration to demonstrate the fact that its preceeding layer is fully connected. Input neurons just pass through what they are provided. In addition to the input layer is typically an additional bias term. It is useful to have because it offsets the starting position of the regressor.\n",
    "\n",
    "In the case above, this perceptron takes a single input and outputs 3 different binary classes. So this on it's own can functions as a multilabel classifier. The output from the fully connected layer can be computed efficiently using linear algebra.\n",
    "\n",
    "$$h_{\\mathbf{W}, \\mathbf{b}}(\\mathbf{X})=\\phi(\\mathbf{X W}+\\mathbf{b})$$\n",
    "\n",
    "One question I had from before was, \"What are all those arrows signifing? What do they actually mean?\". The answer to that is that they are just resembing linear algebra operations. The equation above summarizes it all. It can be rewritten below as follows as well.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{bmatrix}\n",
    "        h_{11} & h_{12} & h_{13} & \\dots  & h_{1k} \\\\\n",
    "        h_{21} & h_{22} & h_{23} & \\dots  & h_{2k} \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        h_{m1} & h_{m2} & h_{m3} & \\dots  & h_{mk}\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\phi \\left(\n",
    "    \\begin{bmatrix}\n",
    "        x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "        x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "    \\end{bmatrix}\n",
    "    %\n",
    "    \\begin{bmatrix}\n",
    "        w_{11} & w_{12} & w_{13} & \\dots  & w_{1k} \\\\\n",
    "        w_{21} & w_{22} & w_{23} & \\dots  & w_{2k} \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        w_{n1} & w_{n2} & w_{n3} & \\dots  & w_{nk}\n",
    "    \\end{bmatrix}\n",
    "    +\n",
    "    \\begin{bmatrix}\n",
    "        b_{11} \\\\\n",
    "        b_{21} \\\\\n",
    "        \\vdots \\\\\n",
    "        b_{n1}\n",
    "    \\end{bmatrix}\n",
    "    \\right)\n",
    "\\end{equation}\n",
    "\n",
    "* $m$ = number of instances\n",
    "* $n$ = number of features\n",
    "* $k$ = number of artificial nuerons (see output layer = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we had a single TLU take in the input for a single instance and weight vector. Within a perceptron, we have a string of TLU's - but each TLU is operating the same way as before: it takes the weighted input from each of the input nuerons. In addition, rather than taking a single instance, we are now dealing with all the instances in the matrix. So the output itself is a matrix. It contains the output row for each instance passed in. And for each row, the output would be either vector of 1, and 0's: the output from each artificial neuron.\n",
    "\n",
    "Mocking an real example using the example image perceptron above, we can do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[22, 26, 30],\n",
       "        [46, 54, 62],\n",
       "        [46, 54, 62],\n",
       "        [46, 54, 62]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (mxk) = (mxn) x (nxk) + (nx1)\n",
    "# n=2 features (not including bia)\n",
    "# m=4 examples (arbitrary)\n",
    "# k=3 nuerons (arbitrary, and includes bias)\n",
    "\n",
    "#                                                     1 weight vector extends each            \n",
    "#                                                     feature (including the bias==3) \n",
    "# 4 examples with 2 features                          to the output layer (==3)                    \n",
    "# |---------------- examples -------------------------|----------------- weights ------------------|-------- bias --------|\n",
    "np.matrix([[1, 1, 2], [1, 3, 4], [1, 3, 4], [1, 3, 4]])@np.matrix([[4, 5, 6], [6, 7, 8], [6, 7, 8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we had `[1, 3, 4]` both as duplicate instances, and the matrix operations produce the same results here. The bias vector was merged along with in the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also simplify to show this operates with a single instance. The equation $h_{\\mathbf{W}, \\mathbf{b}}(\\mathbf{X})=\\phi(\\mathbf{X W}+\\mathbf{b})$ remains the same. But its expansion is reinterpreted.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{bmatrix}\n",
    "        h_{11} & h_{12} & h_{13} & \\dots  & h_{1k}\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\phi \\left(\n",
    "    \\begin{bmatrix}\n",
    "        x_{11} & x_{12} & x_{13} & \\dots  & x_{1n}\n",
    "    \\end{bmatrix}\n",
    "    %\n",
    "    \\begin{bmatrix}\n",
    "        w_{11} & w_{12} & w_{13} & \\dots  & w_{1k} \\\\\n",
    "        w_{21} & w_{22} & w_{23} & \\dots  & w_{2k} \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        w_{n1} & w_{n2} & w_{n3} & \\dots  & w_{nk}\n",
    "    \\end{bmatrix}\n",
    "    +\n",
    "    \\begin{bmatrix}\n",
    "        b_{11} \\\\\n",
    "    \\end{bmatrix}\n",
    "    \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Notice how the weight matrix stays the same. It define the architecture of the perceptron itself. Additionally, the output to that instance remained the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[22, 26, 30]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matrix([[1, 1, 2]])@np.matrix([[4, 5, 6], [6, 7, 8], [6, 7, 8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good question is: why do we need many artificial neurons? Why isn't a single TLU enough? The answer to that question to that is so we can encode many more different things. The output could serve as an interpreter for a multilabeled output. For example, we many want to encode an output of `[1, 0, 0]`, or even something more complex like an image where each out in the matrix array output a pixel intensity value of that image.\n",
    "\n",
    "For more clarity, we'll describe each variable and their significance more closely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $X$ is a $m \\text{ x } n$ the input matrix. It contains $m$ rows for each sample, each containing $n$ features.\n",
    "* $W$ is the weight matrix that represents all the learned weights. There is a learned weight vector for each input neuron, with exception to the bias neuron, and a column for every artificial neuron. Hence, in the image example above, the weight is $2 \\text{ x } 3$, with 2 rows for each feature and 3 columns for each artificial neuron. \n",
    "* $b$ is the bias vector that contains all the weights from the bias neuron to the artificial neuron. The bias vector is nearly identical to a weight vector. We are just artificially putting in bias neuron because we want to leverage additional control.\n",
    "* $\\phi$ is the activiation function is intend to be agnostic to many different types. For example, previously we learned about the step function.\n",
    "\n",
    "The most important thing to understand with the perceptron is that it is just adds more TLUs. In the perceptron image example, suppose were we to take out two artificial nuerons in the output layer. The result would be a TLU, and we know that a TLU fundamentally operates by weighting features and summing them to produce a single number. Now add another artifical neuron here, and give it a different set of weights. The only difference here would be that the output would be different only if the weights were different (hence the importantance of initially randomizing the weights, but more on that later).\n",
    "\n",
    "Ultimately, the artificial neurons operate independently. So wrapping it up in a bigger picture, we can think of the artificial neurons of have their own character or interpretative ability on some inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Training a perception is inspired by Hebb's rule: \"neurons that fire together wire together\". Mathematically, this transates to, when two neurons have the same output, the weight between the two neurons should be increased to signify their relationship. Perceptrons will train on this central idea, by strengthing or increasing the weights between other neuron connections that help reduce the overall training error. The learning rule is summarized below:\n",
    "\n",
    "$$w_{i, j}^{(\\text { next step })}=w_{i, j}+\\eta\\left(y_{j}-\\widehat{y}_{j}\\right) x_{i}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets break this equation down:\n",
    "\n",
    "* $w_{i,j}$ is the index i, j in the weight matrix. Visually, this is the connection weight between the ith input neuron and the jth output neuron.\n",
    "* $x_i$ is the ith training instance.\n",
    "* $\\hat{y_j}$ is output of the jth output neuron (for the ith training instance).\n",
    "* $y_j$ is the target (true) output for the jth output nueron (for the ith training instance).\n",
    "* $\\eta$ is the learning rate.\n",
    "\n",
    "In english this say that the weight between input neuron i and output neuron j is changes proportional the to difference between the true and expected output j for a particular training instance.\n",
    "\n",
    "The decision boundary for a perceptron is linear, and we can demonstrate this with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
       "           fit_intercept=True, max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "           penalty=None, random_state=0, shuffle=True, tol=0.001,\n",
       "           validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int)  # Iris Setosa?\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11cea9da0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAECCAYAAADuGCyPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXRc5Znn8e9TpSrJkrV4l7zvYDBg44UQjG0MNCZ4hXQmSQ/dJKfjTp9OT5h0JuF090xmMjNnejmnT3M6Pc1hkkx3ZgjJJAbsJuyEhC3el2BsY2xsg8HGG5bkTdbyzh9VsmW5rlRXulX3VtXvc44P0qvre58S9uOr976/es05h4iIFL5Y2AWIiEgw1NBFRIqEGrqISJFQQxcRKRJq6CIiRUINXUSkSPTa0M1sjJm9YmY7zextM/t6hmMWmlmjmW1L//pPuSlXRES8lGVxTBvwZ865LWZWDWw2sxedczu7Hfeac25J8CWKiEg2er1Dd84dds5tSX/cDOwCRuW6MBER8cfXHLqZjQdmAuszfPlmM9tuZs+a2bUB1CYiIj5kM+UCgJkNBFYDDzrnmrp9eQswzjl32sw+AzwFTMlwjv8D3AtQnqyoHDl8dJ8LF5Hi9WGb99dGZdm1ejpHf88dpv2H9jrnXMabccvmvVzMLAE8DTzvnPu7LI4/AMx2zh33OmbimCnuuw8+3Ou1RaT0/O0JR2PHleO1MfgPQ6xf5zAgU9fzc+4w3f/Ne84656oyfS2bVS4G/ADY5dXMzaw+fRxmNjd93hN9L1lEStmdVZDoNpZIj/f3HHPK+3/uqMrmB4xbgPuBt8xsW3rsz4GxAM65R4DPAn9sZm3AOeDzTm/jKCJ9NKMidR/94hlo7EjdPd9Z1Tne/3OMPd+/c0dVrw3dOfc6qZ9Sejrme8D3gipKRGRGhTGjIjfnCOLcUaSkqIhIkSiAZ7oikk/b8jwdsbbJsbEl9aDSSM1xL6sp/OmPMOgOXUQu2nbesaaZi6tDGjtgTXNqPBfWNjk2tFxadeKADS2pcfFPDV1ELnrxDLR2G2tNj+fCxhZ/49IzNXQRuSjTuu2exvvL6z5c9+d9o4YuIhfVenQEr/H+8pop1wx636ihi8hFQQR6/JhT7m9ceqZVLiJyURCBHj+W1RholUtg1NBF5DL5Dt0sqzGW5e9yRU1TLiIiRUJ36CJyGa9gUaZxIOtj/U7b+DmH3+vlOzwVhM6akyMmV3odo4YuIhd1Bos616J3Bovev+DY2nL5+JPNqXnvjiyOXdMM4LJuml51ZDqHn2P7cnwUXFZzDyVqykVELvIKFm1suXK8nUvNvLdj/YaT/ASc/Iah8h2eCkKmmjNRQxeRi7wCRH6CPl7H+gkn+Qk4+Q1D5Ts8FYRsa1NDF5GLvAJEfiYivI71E07yE3DyG4bKd3gqCNnWFuGXICL55meXnzhXNpCgdgTyE3DyG4bKd3gqCJlqzkQPRUXkIr+7/EDmVS793RHIT8DJbxgq3+GpIHSt+XAP819ZbRKdC9okWkTEv35tEi0iIoVBUy4iEqpSCAV5Cfq1qKGLSGhKIRTkJRevRVMuIhKaUggFecnFa1FDF5HQlEIoyEsuXosauoiEphRCQV5y8VoK8NsgIsWiFEJBXnLxWvRQVERCUwqhIC+5eC1q6CISKr87JOV7R6VcCvq1aMpFRKRI6A5dRAKX712Poh42yld9augiEig/ux490Zx6u932LI4NaseifMtnfZpyEZFA+dn1qINLzby3Y4PasSjf8lmfGrqIBCrfux5FPWyUz/rU0EUkUPne9SjqYaN81heRlywixcLPrkcxUjsfZXNsUDsW5Vs+69NDUREJVL53PYp62Cif9amhi0jgvAIz3uPZH+vnelGRr/o05SIiUiR6vUM3szHAj4ARpB4+P+qce7jbMQY8DHwGOAs84JzbEny5ItIXQQR6oh7eybcgvh9h7FjUBvyZc26LmVUDm83sRefczi7H3A1MSf+6Cfin9H8vY2argFUAE0ZP7nPRIpK9TMGWJ5tTd2cdXcZ6CvT4CfqUgiDCQqHsWOScO9x5t+2cawZ2AaO6HbYc+JFLWQfUmVlDhnM96pyb7Zyb3adqRcS3TMGWdi418049BXr8BH1KQRBhodB3LDKz8cBMYH23L40CPujy+SGubPqY2Soz22Rmm5rONPqrVET6xE+AxSvQ4yfoUwqCCAuFumORmQ0EVgMPOuea+nKxrnfoNVW1fTmFiPjkJ8Di9YO+n6BPKQgiLBTajkVmliDVzB9zzj2R4ZAPgTFdPh+dHhORkGUKtsS58i9/T4EeP0GfUhBEWCiUHYvSK1h+AOxyzv2dx2Frga+Z2U9IPQxtdM4d7ntZIhIUr2AL+Av0ZBv0KQVBhIXC2rHoFuB+4C0z25Ye+3NgLIBz7hHgGVJLFveSWrb4pT5XJCKBCyLQE/XwTr4F8f0I+nvaa0N3zr1OL++r45xzwJ8EVZSIiPin6L9IifIKtaxtcmxsSa1sMVLz58tqCnOnIC+FWndv1NBFSpBXqGXLWcd7XXaccMCGFqDJXdHUo75TkJdCrTsbJbroSKS0eYVa3uu+fVDaxpbszxH1sFGh1p0NNXSREuQ3vJIpWBT1nYK8FGrd2VBDFylBfsMrmSYior5TkJdCrTsbRfASRMQvr1DLxO7bB6XNKc/+HFEPGxVq3dnQQ1GREtRTqCXbVS5R3ynIS6HWnQ01dJES5RVqWVZjLOvnOaKuUOvujaZcRESKhO7QRYrID09evo58Yhy+PDg6YaFMdYxNZn5PGS9+6ovirkK5pIYuUiS6N3NIrSv/62OO5i5jYYWF1ja51HW71dF1rLfr+akvqrsK5ZKmXESKhFcoqDnzcN7DQpmul0lP1/NTX1R3FcolNXSREpXvsJDXrkd+rudnPKq7CuWSGrpIicp3WMjPBIXfOjKNR3VXoVyKaFki4pdXKKja4/h8h4UyXS+Tnq7np76o7iqUS2roIkXiy4PtiqY+MQ7fHmbMLb90h2zA3B7CQsurL92B1sZgeXUwqzqW1WSu47M+ruenviBeSy6/H7lgqb0p8m/imCnuuw8+HMq1RUQK1f3fvOescy7jzwi6QxcRKRJahy4SEfkOsBRSYEayo4YuEgH5DrAUWmBGsqMpF5EIyHeApdACM5IdNXSRCMh3gKXQAjOSHTV0kQjId4Cl0AIzkh397xOJgHwHWAotMCPZ0UNRkQjI9y46xbxrTylTQxeJiHzvolOsu/aUMk25iIgUCd2hi0REvnfiCaKOMES9vjCpoYtEQL534gmijjBEvb6wacpFJALyvRNPEHWEIer1hU0NXSQC8r0TTxB1hCHq9YVNDV0kAvK9E08QdYQh6vWFTd8GkQjI9048QdQRhqjXFzY9FBWJAD9Bn1yGgqIeOIp6fWFTQxeJCD9Bn1yGgqIeOIp6fWHSlIuISJHo9Q7dzH4ILAGOOuemZ/j6QmANsD899IRz7rse51oFrAKYMHpyH0sWiZ4gwi5/fczR3OXzauCuajKe1+/11jY5NraAI7U585xyGJsM5twSHdlMufwz8D3gRz0c85pzbklvJ3LOPQo8CqlNorMpUCTqggi7dG/mAM3Az7sMdp73/QuOrS3ZX29tk2NDy6XPHbChBTa1QEe3c/g9t0RLr1MuzrlXgZN5qEWkIAURdunezL20Ahtb/F1vY0vm8e5Lt/tybomWoObQbzaz7Wb2rJld63WQma0ys01mtqnpTGNAlxYJV77DLl4/2npdz8+Pwn7PLdESREPfAoxzzt0A/APwlNeBzrlHnXOznXOza6pqA7i0SPjyHXbxmvjwup6fiRK/55Zo6ff/Judck3PudPrjZ4CEmQ3td2UiBSKIsEt1lsclSD3Q9HO9OeWZx7v/5e/LuSVa+t3QzazezCz98dz0OU/097wihWJGhbG8+tJdbG0Mllf7Wxny7WF2RVOvBj6b4bzLavxdb1mNMbf80t23AXPL4d4Azi3Rks2yxceBhcBQMzsEfIf0P+LOuUeAzwJ/bGZtwDng8845rWCRkhJE2OXbwzI3zUzn9Xu9ZTXGshydW6Kj14bunPtCL1//HqlljSIiEiJF/6Xk5TtIkynks6zG385EXucoxFBQIdYcVWroUtLyvQOOV8iHJndFU/eqbctZx3vtV57j+EnHB+2FFQrSDkTB0mIkKWn53gHHK+STadyrtq7NvKv32gsvFKQdiIKlhi4lLSqhoEzjQdUQ5VCQdiAKlhq6lLSohIIyjQdVQ5RDQdqBKFj6tklJy/cOOF4hn0zjXrVNjGc+x8R44YWCtANRsPRQVEpavnfAWVZjkOUql55qK5ZVLtqBKFgWVgZo4pgp7rsPPhzKtUVECtX937znrHMu488wmnIRESkSWU+5mNmdwOeAf3TObTOzVekNK0RCk6sphh+evHyt98Q4fHmw91RHpnHA17RIpnHIvKuQSCZ+5tC/DPwx8JdmNhiYkZuSRLKTq1BK92YOqTXe3XcV6gz07D/uOOauHCfDsV7hn0w7BT3RnPqHoL3bsQrdiBc/Uy7NzrlTzrlvAr8DzMlRTSJZyVUoxSu447Wr0DEfj6G8wj+Zdgrq4FIz73qsQjfixU9D/0XnB865h+h5j1GRnCumUIqfpQmF+PokP3pt6Gb2sJmZc25N13Hn3D/kriyR3hVTKMXPBEohvj7Jj2z+aDQDa82sEsDM7jKzN3JblkjvchVK8QrueO0q5PE25p7nzlRzpp2CYkD3UhS6kZ702tCdc38JPA78Ot3IvwE8lOvCRHoTxE5BmXx5sF3R1CfGUxtQZNr55+tDM49nGvvy4Mw1Z9op6N5qWKndg8SHXoNFZnY78Jek/kw2AMucc+/098IKFomI+NffYNFfAP/RObeQ1HZzPzWzRQHWJyIiAchmC7pFXT5+y8zuBlYDn85lYSL5EkSgx0/AKYgwVKG9Z4vkh+8353LOHU5Pw4gUvEzhJL+BHj8BpyDCUNrlR7z0aQGUc+5c0IWIhCFTOMlvoMdPwCmIMJR2+REvWtEqJc1PSMdvkCnTeBBhqGIKVEmw1NClpPkJ6fgNMmUaDyIMVUyBKgmW/ghIScsUTvIb6PETcAoiDKVdfsSLdiySkua1Yw5kv8rFz647QezQo11+xIsaupS8GRXGjIpM4/0/R3+PzeU5pPhoykVEpEjoDl36pBCDLX52Cor6axHJRA1dfCvEYItXzZl2Cor6axHxoikX8a0Qgy1eNWfaKSjqr0XES2gN/fzZFk5/4rWpl0RZIQZbvGrzeq/RKL8WES+hNfRGa+exN3eyY91uOjr0t6eQFGKwxas2r0mVKL8WES+h/bEtH3yB2FfP8fyQFtauXsepo41hlSI+FWKwxavmTDsFRf21iHgJ7aFoVaKFpdO2sXNEA5smT+bIi+8wt7yKG265mniZxx5gEgmFGGzpqeaxWuUiRaLXHYsCvZjZKmAVwHU3jJr14E9n0dh6hlOtA/jVu1fT+MoQJmxp5o7bplM7rDZvdYmIFIr+7lgUGOfco8652c652WbGZ0YsYMHQWYwoP8fs8QdoubaDoyMq2fHm7nyWJSJSFHqdcjGzHwJLgKPOuekZvm7Aw8BngLPAA865LdkWMLJiBAMTZZTFOiAGLm60t+fvpwYJz9omx8aW1EoTIzWfvawm81SHn2NzuSOQQkgSZdncof8zsLiHr98NTEn/WgX8k/8iYgwpP801k96n+Xpjjzn2/XY/+ZwOkvxa2+TY0HJp2aADNrSkxvtzbGeAqHPZYWdQaNv57P8seZ1jbVP/zy2SS702dOfcq8DJHg5ZDvzIpawD6syswU8Rnxp0E9VlZdwyah+L79lMy1c6eLrjFM8/tYGzjUp4FKONLdmP+zk2lzsCKYQkURfEHPoo4IMunx9Kj13BzFaZ2SYz23Ti+KW/BQPLqllRv4iZtVMZVdnIipmbmPQHB9h+VzmP/fq37N70Lq5Dd0HFxOv/ZqZxP8fmckcghZAk6kJ7KDpk6OUPac2MadWTWDriVuoSCebW72fJ3RtpW9XKMwPO8vST62k+qWRpsfCadc407ufYXO4IpBCSRF0QfxQ/BMZ0+Xx0eqxPqhIDWVZ/G3MHXcuIitOsuH4rw+87ws6bq3nyF1s4+eGJfhcs4ZtTnv24n2NzuSOQQkgSdUE09LXA71vKp4BG59zh/pzQzJhcNY4l9bdQk2hjVO0pLgyB1poBnFBDLwrLaoy55Zfueg2Y67Fyxc+xMyqM5dWX7pprY7C82v+OQJnOsaym/+cWyaVsli0+DiwEhprZIeA7pG9UnHOPAM+QWrK4l9SyxS8FVVxFbABxg7J4OyQ66EgYLee7P5aSQrWsxliWg2NzuSOQdgqSKOu1oTvnvtDL1x3wJ4FV1EXcykhYgquqj5CY186b1VN55cUWPnl2CzfdNp1kRTIXlxURKUiR3uDCzLhj2O1sbXyHGAdouLmR9WMn8OZrYzj47GbmTxvL2KszLqgRESk5kX8+H7c4s+uuYfHwT1OXgIVj93DH8i2c+BI8dfoYL63dwLnT58IuU0QkdJG+Q++qLlnHyoZF7GjeS4x9DJ+zic2jx7H5jfEcfHErt00eycTrxoddpohIaCJ/h95VzGJcXzOVe+rnMSgBnx61j6GzjnNidi3rtx6k6XhT2CWKiISmoBp6p+qyauYPuYFkrIOK5AU6kkZHWYzW8xfCLk1EJDQF2dABzFKlj689gY06R9OoSvbuPER7W3vIlYmIhKNgG3pt2WDqElVcVfsx983fQOUDJ3n1KmP1U+s49sGxsMsTEcm7gm3oMYtxx7CFLBwymyHJVhZP3cHN/2YH738xyc/2H+Q3L2yn7UJb2GWKiORNwTb0TvUVw1k5chGTKkdzde3H3HfrBgb94XFevw5+9vR6Ptp3JOwSRUTyouAbOkCZlfGpwddxx/CbGJTs4PYJu5h/33aO3F/G6mMf8eovNnNBD0xFpMgVzDr0bAxNDmFlwyK2Ne4hxn4abm5kw9iJ/Ob1Mex7eiO3XzuOsdNGh12miEhOFMUdeldxizOrbhqLh99CXcKYP3YPdy7fwvEHkqxpPMav1mzQ3LqIFKWia+id6pK1rGhYxDXVYxk38CRXTfmQpmlJ9l9o48i+fr27r4hIJBVtQ4fUSpirBo6nIu6IxzogbjgzOtq1Z5iIFJ+ibuiQegveGDC19igN1x7hk5mVbNz1IScPfxJ2aSIigSqqh6KZJGPl3Fh7I2fatnPPVdvZPaKB9VOmcPTFvczZVcHM+dcQL4uHXaaISL8V/R06wKgBDdw3chHjKhu4ZtBH3LdwAwNXneKVKY7Va9Zz9ODRsEsUEem3kmjoAIlYknlDZnDb0DkMTrZy1+Qd3PK7b3Hoiwl+9v4HvPHcNlovaHs7ESlcJdPQO9VXDGPlyEVMrhrD1NqPWT5vI+fubWVjfYznf/IG7a1a0igihankGjqkkqVzB01n7qAp1CXOUVt7jguD45zucJw7fT7s8kRE+qQkG3qnqrJKzGBAopWOJLQlE7ScbQm7LBGRPinphj4wXkMcWDRmFzPufodD9w5k9da97NzwDq7DhV2eiIgvJd3Qq8qquXv4bdSVJblx+AGWL94Aq87xXM151j6xTlvaiUhBKfp16L2pLKtkaf1CDpw7xPqTb7N0+jbeHjmSzeuncOTl3dw0oJrrP30VsXhJ/9snIgVAXQowMyZUjmFlw0IGJyq4YcghVt6xnrKvnOalhgs8uXodnxxRslREok0NvYvyeAX31C9kwdAbGVZxhgWT3qF9TisfTqpm3Qvbwy5PRKRHaugZjKyoZ9rAsVTE24glHS5htLbqDb1EJNrU0D3ELU4y1sbkEYc5P66D44Mr2L/jIM5p9YuIRFPJPhR97u1qHvn1MD5uKmNETRtfXXCMxdc2X/z6+Mrx7Dn9Ebc07GPCkuO8NuFq1r50kqvXfMytt0+nsroyxOpFRK5Ukg39uber+atn6znflvoB5UhTgr96th7gYlMfEK9kaf0idp9+jxjvsnLWJraNGcPWNyZw6OXtzBszjKkzJmExC+11iIh0VZJTLo/8etjFZt7pfFuMR3497LKxmMW4pnoyS+tvpS5Rxtz6Ayy5ZxOtf9TGM8nT/OLJdZz+pBkRkSgoyYb+cVPmH0y8xqvKBrK8/jbm1E2jvqKZZddvYdrv72PHkgp+/OZOdqxTslREwleSDX1ETeZ3VPQah9Ra9SkDx7OiYQGDypLMGnaQ5XdtwP7oHM8POs+a1b/h1NHGXJUsItKrvDZ0M1tlZpvMbFM+r9vdVxcco6Ls8mWIFWUdfHXBsV5/b0W8kiX1C7l58PUMKz/H0mu2Mf0L77J7aRU/27CbneveyVXZIiI9ymtDd8496pyb7Zybnc/rdrf42mYeuvsI9TWtGI76mlYeuvvIZatcemJmjK8czb0NtzE4EWfmsA9ITj5L09SBvP3b92m7oPdUF5H8K8lVLpBq6tk2cC/JeDnX107mjZO7iMUc7QaYaa26iIQiqzt0M1tsZu+Y2V4zeyjD1x8ws2Nmti396w+DLzWaymIJzBzzxu4hPquZwzOG8toL2/S+6iKSd73eoZtZHPhH4E7gELDRzNY653Z2O/Snzrmv5aDGvOotcNTd8ORIrh54hg63jxE3bWTj2PFsfH0s+5/bzMLJDUy8bjxmWqsuIrmXzR36XGCvc+4959wF4CfA8tyWFY7OwNGRpgQOuxg4eu7tas/fE7MY19dcxWdGzKOuLMa8UXtZvHQz577ieLrjFC+s2cDZprN5fBUiUqqyaeijgA+6fH4oPdbdfWb2WzP7uZmNCaS6PMs2cJRJTVkNKxoWMbN2KqMqG1l542Ym/cEBtt1ZzmO/2s7uzXu1Vl1EciqoVS7/Cox3zl0PvAj8S6aDui5bPHH8TECXDo7fwFF3Zsa06kksHXErtYky5tbvZ/6dOzi6vJJXTjay+SW9Ba+I5E42Df1DoOsd9+j02EXOuRPOuc6ngN8HZmU6Uddli0OGVvWl3pzqS+Aok6pEKlk6trKWYQNOE69ro21QghMKHolIDmXT0DcCU8xsgpklgc8Da7seYGYNXT5dBuwKrsT86U/gqDszozxWTsLaSSZbaSs3znU4rVEXkZzpdS7BOddmZl8DngfiwA+dc2+b2XeBTc65tcC/M7NlQBtwEngghzXnTOdqFj+rXHoyIjmU6rKPWTF9M+uqJ/Pu0Hp+9q8bWHjjJBomjAiydBERLKwQzA0zx7jnX/n3oVw7n462HOPVE9u40NHGe01DeXP7VBIvJpjRBDfdNp1kRTLsEkWkgNz/zXvOOucyzlmXbFI0X4aXD2NlwyK2nnqHGAdo+HQj68dN4M3XxnDw2c3MnzaWsVdnWjQkIuJPQTZ0v+Gfrz0+ik0HL/2DNnvcGZZc35TxHH7One2xcYsze9A1TK4aycvHN7Fw7B4mLT/GqxOn8dRLx7hu7YfccucMkgN0ty4ifVdwDT2b3Ya6utTML6U1Nx2sYvPBKlx6rPMcvz00gGfeqs3q3H7rAKhL1rGyYRE7mvcSt72cvXYfr5y4jr0vGMM2vsv0+df29dsiIlJ474fuN/zTvZmn2MVm3vUcT22ry/rcfQ0hpZKlU6lJQCLWjiXAlcVo1eoXEemngmvo/Q3/9MQryJnp3P2vwxhe0UzDuOM0T06w7+RpPjlyKsvfKyJypYJr6EGFfzLx2u8507n7W8fM2hupSbSw5Krt3Pj5nbz3uQp+8va7bH5lB+1t7VnXLCLSqeAaut/wz+xxZ4Dut94O6zZWUdbBihmnsj53f0NI9eUjuK/hdoYkBzB98Efcu2gDlX94il9ObGf1U+s49sHxrM4jItKp4Bq6392GvveFD7s09dSv2ePO8J2lh684x7fuOpr1ufu76xFAIpbk7hELuG3obIYkW1k8dQc3f24H738xyc/2H+A3L2xXslREsqZgUUS0uTY2fbKLA2c/oLmtnHUfTOLQrxsY/ZvTLLhhEiMn1YddoohEQE/BooK7Qy9WZVbGpwZfxx3Db2JQsoPbJ+xi/n3bOXJ/GauPfcSrz2zmwvkLYZcpIhFWcOvQAf7m+eE8ta2ODpd6kLlixim+ddfRjAGi733hw5yEhXJlaHIIKxsWsa1xDzH203BzI6+OmspvfjmSA6vX8Zn50xg6pvf3ZxeR0lNwDf1vnh/OE1vr6Fxb3uHgia11vLqniuNnEnQPEH3hf43jcGMyZ2GhXIhbnFl105hUOZJfnXiD6xs+5OCU4ZzeX8POjfuYr4YuIhkU3JTLU9suNfNL7Ipm3jm+/0R5zsNCuVKXrKWqLEHMXGpf0rhp1yMR8VRwDT2ofpabsFDw4hZjWHkTc6ftoXluB7ury9j++i462jt6/80iUlIKrqF7hX/8ykVYKBfmDZ5HbdkAbhh6iJV3rqfsK828VH+Bp55cz8nDn4RWl4hET8E19BUzTpEpKDS0qjXj+IQhLXkLC+VCMlbOPSMWMG/wDAYnz7Pk6u3M+uIu9v5ukp/u3MumX76lZKmIAAXY0L9111HunXmKmKVCQjFz3DvzFE//6f6MAaLHv3Iwr2GhXDAzxlSO5L6RtzOusoFrB33EfbdtYOCqU7wypYPVa9Zz9ODRUGsUkfApWFSAjpw/xmsnttLq2nn1oynseWUCI1/8hMU3TaZ+ogJIIsVMwaIiU18xjOUNC6gu62DowNO01UB7VTlNRxvDLk1EQhTa8o3dRypY8T8n9hrcyRQi2vr+APafKL94zIQhLTz+lYPc8tdTaHeXnprGzfHGt99lwd9OpqX90r9d5fEOqiva00sdU4ZWtfL0n+4vmBBS3OKYQWX8ArEBbbQOiNPYeBbn0kscRaTkhDblUt4wxTX8wd9TUdbhOU/dPUSU0llv9zGXHst2/MpzDEy209YRu2wtuld93UNIPR2bC845Xj7+S461tHDkXA2v7b6alpcHMnXPORbcfh0D6wbmvAYRyb9IT7n0FNzxChF5j/kdv3zs9IV4wYSQzIzbhy5iVt00GgY0s2LGZq7+/fd4654B/Pj1t9m54R2FkERKTOgNHbyDO1HpR1ENIZkZUwdOYFn9AurKkswafoDlizfAqjhovbIAAAXESURBVHM8V3OetU+so+l4U97qEZFwRaKhewV3ggoR9VfUQ0iVZZUsrV/IzYOvY1j5WZZO38b1v7eHXcsH8PjG3Wx/fRdhTa2JSP6E3tB7Cu54hYi8x/yOXz42MNlesCEkM2NC5RhW1C9gcCLBDUM/YOR1x/hkRhVvHzjOqY+1X6lIsQu1ofcW3PEKEU0Y0kLXZj1hSAvrHnqXuLnLxuPmWPfQu5THOy4bL493dEmWpn4NrWrlpW/sK/gQUkXZAD495AYSMUeirB1LGB0xaG/VzkcixU7BoiLU1PoJLx57g32nh/Dqb6cRe6mC6YcvcMsdN1BeWd77CUQksiK9ykWCV11Wx1UDpzC26hPum7OR0Q98wMZ5FTz23Gb273hf8+kiRSoyG1wEEdLx2snIa7xYmRnTa65izIAGXj62gXmj9zJx+TFem3w1a186wbQ1R5h3+3QqqyvDLlVEAhSJhh7ETkFeOxldSpVePg4UdVMHqE3UsKJhEbtOv0eMd1k5axNbR49ly5vjef/l7dw6ZjhTZ05UslSkSERiyiWIkI5XCKlrM+86njq++MUsxrXVk1lafyt1iTJuaniPG297l8N3VvPa+8fZu2lf2CWKSEDy+lDUzFYBq9KfzgLOAiRHTK68oucCOLjw8d6z2Zy7p3P099z9UAZks7wk2+OiKOjao/a9CLOefF07at/zTPpbYyG8xmwNcM5lvBkPbZWLmZ3xelJbLMxsk3NudlDHRVHQtUftexFmPfm6dtS+55n0t8ZCeI3Z6um1RGLKRURE+k8NXUSkSITZ0J8I8dr58mjAx0VR0LVH7XsRZj35unbUvueZ9LfGQniN2fJ8LaHNoYuISLA05SIiUiTU0EVEikTeG7qZ/dDMjprZjnxfO1/MbIyZvWJmO83sbTP7etg1BcnMKsxsg5ltT7++/xJ2TbliZnEz22pmT4ddSy6Y2QEze8vMtpnZprDrCZqZ1ZnZz81st5ntMrObw64pl/I+h25m84HTwI+cc9PzevE8MbMGoME5t8XMqoHNwArn3M6QSwuEpd4roMo5d9rMEsDrwNedc+tCLi1wZvYNYDZQ45xbEnY9QTOzA8Bs59zxsGvJBTP7F+A159z3zSwJVDrninZzgLzfoTvnXgVO5vu6+eScO+yc25L+uBnYBYwKt6rguJTT6U8T6V9F93TdzEYD9wDfD7sW8c/MaoH5wA8AnHMXirmZg+bQc87MxgMzgfXhVhKs9FTENuAo8KJzrqheX9rfA98COno7sIA54AUz25x+a45iMgE4Bvzv9LTZ982sqNPpaug5ZGYDgdXAg865otqt2TnX7pybAYwG5ppZUU2fmdkS4KhzbnPYteTYPOfcjcDdwJ+kp0SLRRlwI/BPzrmZwBngoXBLyi019BxJzy2vBh5zzhVtiCr9I+wrwOKwa+mP9EPsO9Mf/zfgPwPL0nPMPwEWmdn/Da/C3HDOfZj+71HgSWBuuBUF6hBwqMtPjz8n1eCLlhp6DqQfGv4A2OWc+7uw6wmamQ0zs7r0xwOAO4Hd4VbVb98B/sLMfo/UFNlNzrnRzrnxwOeBXzrn/m2YBQbNzKrSD+1JT0X8DlA0q8+cc0eAD8zsqvTQ7UBRLEzwkvcNLszscWAhMNTMDgHfcc79IN915NgtwP3AW+l5ZoA/d849E2JNQWoA/sXM4qRuCv6fc66gl/U5515N/0P8DWChc67dzCYCfwFMBsLd/Ts3RgBPpjc4KQN+7Jx7LtySAvenwGPpFS7vAV8KuZ6cUvRfBDCz60hNkZ1wzt3c7Ws/d859NpzKRLKnKRcpeencwGPAcuC0mRX08wApXWroUtLMrJLUO3/+mXNuF/BfSc2nixQcTbmIeDCzIcB/J/XQ9/vOuf8RckkiPVJDFxEpEppyEREpEmroIiJFQg1dRKRIqKGLiBQJNXQRkSKhhi4iUiTU0EVEioQauohIkVBDFxEpEv8fG9SfFk3qMjYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mltoolbox.draw.classification import plot_2d_decision_boundary_supervised\n",
    "\n",
    "\n",
    "plot_2d_decision_boundary_supervised([per_clf], X, y, 100, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons are actually equivalent to `SGDClassifier` with `loss=\"perceptron\", learning_rate=\"constant\", eta0=1` (the learning rate), and` penalty=None` (no regularization). But unlike logistic classifiers, perceptons do not produce class probabilities. Instead, they will hard classify simplify based off the step function.\n",
    "\n",
    "One of the hard limitations of perceptons where the inability to classify xor type problem due to the linear decision boundaries. For example, we cannot draw a decision boundary through the following truth table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>a ^ b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b  a ^ b\n",
       "0  1  0      1\n",
       "1  0  0      0\n",
       "2  0  1      1\n",
       "3  1  1      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame([[1, 0, 1], [0, 0, 0], [0, 1, 1], [1, 1, 0]], columns=['a', 'b', 'a ^ b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Trying Linearly Seperating These')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y, color = [1,0,0,1], [0,0,1,1], ['r', 'b', 'r', 'b']\n",
    "plt.scatter(x=X, y=y, color=color)\n",
    "plt.title('Trying Linearly Seperating These')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = np.c_[np.array(X).T, np.array(y).T], [1, 0, 1, 0]\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "per_clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptrons\n",
    "\n",
    "This limitation can be lifted if you stack perceptrons. The creation of this new nueral network architecture is Multilayer Percepton. MLP's are one example of feed forward neural networks or FNN.\n",
    "\n",
    "<img src=\"../../../../assets/1-supervised-learning/mlp.png\" style=\"width: 750px; margin:auto 0;display: block;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "mlp_clf = MLPClassifier()\n",
    "mlp_clf.fit(X, y)\n",
    "mlp_clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'The MLP is able to Solve the XOR Problem')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_2d_decision_boundary_supervised([mlp_clf], X, y, 100, 100)\n",
    "plt.title('The MLP is able to Solve the XOR Problem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, when any form of artificial neural network contains many hidden layers, we obtain a deep neural netwrok or DNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Perceptrons could be trained using the same learning dynammics from a single TLU. And a single TLU bases its foundations from basic linear or logistic regression.\n",
    "\n",
    "In MLP, there are several layers and it was difficult to understand how the weights could be cross communicated throughout the network during training. Backpropagation was introduced a way to solve this problem.\n",
    "\n",
    "##### The BackPropagation Algorithm\n",
    "\n",
    "Lets first walk through how the algorithm intuitively works.\n",
    "\n",
    "\n",
    "**Forward Pass**\n",
    "1. Your neural network architecture is constructed and initialized with random weights between 0 and 1. As discussed before the random weight initialization is done to promote diversity in the training process between artificial neurons.\n",
    "2. A batch of inputs pass through the network (say 32). When all the instances pass through the network, 1 epoch is passed. The network runs through 1 or more epoches in batch mode.\n",
    "3. The 32 instances are passed as a matrix through the networks input layer (which are pass through, so it sends these off the the first hidden layer.) The first hidden layer will then compute the outputs using its associated weights for all the instances in the mini batch. This process is repeated from layer to layer until the output is obtained in the final layer.\n",
    "\n",
    "Note: during this step, all intermediate results are preserved since they are needed for the backward pass.\n",
    "\n",
    "**Backward Pass**\n",
    "4. First the error is measure using some metric or loss function that compares what the nueral network go, and what the desired output should be. This to, is done for all 32 instances.\n",
    "5. Next, each output neuron computes how much it contributed to the error by applying the chain rule. Recall that a neural network will feed the input of one neuron to the next. Mathamatically, we can summarize these process as one giant composite function. The chain rule operates composite functions as follows:\n",
    "\n",
    "$$\\frac{d}{d x}[f(g(x))]=f^{\\prime}(g(x)) g^{\\prime}(x)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. The algorithm then\n",
    "\n",
    "The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain rule—and so on until the algorithm reaches the input layer. As we explained earlier, this reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network (hence the name of the algorithm).\n",
    "\n",
    "Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit in these notes in somewhere:\n",
    "\n",
    "* The number of trainable parameters scales proportionally with the size of the layers. For example, if one layer has 300 neurons and the next layers has 500 nuerons, then the total number of trainable parameters between just these two layers is 300 * 500 = 150000 weights. While this provides the network the flexibility to fit the training data, it also provides a big opportunity to over fit the training data simply due to the number of tunable parameters that almost morph in a kind of state machine for the network. This also explains why neural network require a lot of training data to be performant.\n",
    "\n",
    "\n",
    "### The Architecture of Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "* An example of a multi-layer feed forward neural network.\n",
    "\n",
    "![basic neural network](../assets/figures/nn.png)\n",
    "\n",
    "* The activations of one layer influence the activations in another layer.\n",
    "* There are not a few rules of thumb to follow for the design of hidden layers, but there some design heuristics to follow that can help others achieve the desired results from their nets.\n",
    "* Another neural architecture is called a recurrent neural network. In a recurrent neural network, neurons obtain a limited time to fire before entering an inactive state. This potentially causes other neurons to fire, which eventually reach in active state. Over time this causes a cascading of neurons to fire, which is more akin to how our own brains work.\n",
    "\n",
    "#### Properties\n",
    "\n",
    "* A perception combined to a set of other layer of perceptrons is actually the definition of a linear regressor. So a neural network is generalizes a linear regressor. So what makes a nueral network achieve non-linear optimization? Adding more layers just adds another layer of linear combinations, and does not make this difference. What does make this difference is adding a non linear term, (such as relu or sigmoid) since these are both non-linear aggregators. Of course, adding sigmoid or a relu are not the only option. But the take-away is that if we want to model non-linearity, we'll need a non-linear term, for example $x_1x_2$ which introduces a squared like aggregation of features.\n",
    "* Neural nets can functions as non-linear classifiers. This means that any classification problem such cannot be fit with function $y=b+w_1x_1 + w_2x_2 + \\dots + w_nx_n$. \n",
    "* Stack layers of non-linear terms allows neural nets to model deeper and deeper complex relationships.\n",
    "\n",
    "\n",
    "### A Simple Neural Network to Classify Handwritten Digits\n",
    "\n",
    "![neural network for character recognition](../assets/figures/nn_digits.png)\n",
    "\n",
    "* Input layer: for each pixel intensity in a $28*28=784$ flattened pixel image.\n",
    "* Hidden Layer: will be experimented on.\n",
    "* Output Layer: characterization of the digit, which we will classify by selecting the maximum value (which can be almost treated as a probability). The justification for 10 particular output neurons is empirical. We can use 4 neurons, which can represent $2^4=16$ things (sufficient to representing 10 objects), but it turns out 10 neurons for the output layer is more effective and it makes more intuitive sense.\n",
    "\n",
    "### Learning with Gradient Descent\n",
    "\n",
    "#### Cost Function\n",
    "* A cost functions helps us quantify the performance of our model. The larger the cost, the larger the error. For our cost function we will use MSE, or mean squared error. When $a = Y(x)$, then error is zero, and when $a >> Y(x)$, the error is significantly large than if $a > Y(x)$. This \"punishes\" the network a lot more if it performed significantly more than if it than it performs moderately bad.   \n",
    "* Alternatively, we can quantify loss as the number of misclassified images, but this would make the cost function non-continuous and nondifferentiable. Having a differentiable function make it _easier_ to know what direction and adjustment to the weights to obtain a smaller cost.\n",
    "* There are however, alternatives to the MSE cost function. However, MSE generally works typically well.\n",
    "* In just words, the cost function is a measure of how poorly the neural network performed. In our cost function, what we really want it to reflect a higher cost when the network is wrong, and a lower cost when the network is more of less correct.\n",
    "\n",
    "\\begin{eqnarray}  C(w,b) \\equiv\n",
    "  \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2.\n",
    "\\end{eqnarray}\n",
    "\n",
    "* $w$: all the weights (vector)\n",
    "* $b$: all the biases (vector)\n",
    "* $n$: number of training examples\n",
    "* $x$: training input (28x28 image)\n",
    "* $a$: output of the network\n",
    "* $y(x)$ the true output\n",
    "\n",
    "* Our cost function measures the differences of how close the output was in classifying a particular number. Each number is represented in binary ($[0,1,0,0,0,0,0,0,0]$ represents 2 for example). And the better the network is in getting to as _close_ to those particular set of zeros and ones, the better it performs (as defined by the cost function).\n",
    "\n",
    "![image](../assets/figures/cost_function_ex.png)\n",
    "\n",
    "* We consider the average cost (how well the network performed in classifying all the digits, or some subset of them), to determine its overall performance.\n",
    "* The cost function is parameterized by all the weights and biases the produced its output. For example, if our network has 784, 16, 16, 10 neurons respectfully in the input, hidden layers and output layer, then we would have a total of 13002 parameters to tune.\n",
    "    * $13002 = (784*16) + (16*16) + (16*10) + (16 + 16 + 10)$\n",
    "* The values of these 13002 parameters determine what the output and the performance of the network will be over the thousands of training examples. \n",
    "\n",
    "#### Gradient Descent\n",
    "* For the process of optimizing the weights and biases, gradient descent will be used.\n",
    "* The derivative of a function locates what variable defines the extremum. Partial derivatives help us identify the extremum for with respect to a particular variable, and the gradient identifies the extremum for all the variables defined for that function. In other words, the gradient of a function gives you the direction of steepest descent and it packs together all the information of a partial derivative.\n",
    "* The partial derivative with respect to $w$ and $b$  $(\\frac{\\partial C}{\\partial w}, \\frac{\\partial C}{\\partial b})$ tells us how quickly the cost function when we change the weight and biases.\n",
    "\n",
    "* A short review on gradients: \n",
    "    * $f(x, y) = x^2+y^2$\n",
    "    * $\\frac{\\partial f}{\\partial x } = 2x$\n",
    "    * $\\frac{\\partial f}{\\partial y } = 2y$\n",
    "    * Together, these two functions as a vector produce what is known as a vector field. A particular value in a vector field will point in the direction of steepest descent. For example, if I am at point $(2,1)$ in the function $f(x,y)$ above, then moving towards the direction $[4, 1]$ will keep me on track for greatest accent. The value $[4,1]$, tells us the magnitude of the steepness in each direction. \n",
    "    * It can be shown that moving in the direction of the gradient, the rate at which the function changes is given by the magnitude of that gradient.\n",
    "* In general the gradient is defined as $\\nabla f(x, y) =  [\\frac{\\partial f}{\\partial x }, \\frac{\\partial f}{\\partial y }]$\n",
    "    * Which packs together the partial derivative with respect to every direction into a vector.\n",
    "    \n",
    "\\begin{eqnarray} \n",
    "  \\Delta C \\approx \\frac{\\partial C}{\\partial v_1} \\Delta v_1 +\n",
    "  \\frac{\\partial C}{\\partial v_2} \\Delta v_2.\n",
    "\\end{eqnarray}\n",
    "\n",
    "* We defined the partial change with respect to a particular direction as:\n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\nabla C \\equiv \\left( \\frac{\\partial C}{\\partial v_1}, \n",
    "  \\frac{\\partial C}{\\partial v_2} \\right)^T.\n",
    "\\end{eqnarray}\n",
    "\n",
    "* Sometimes is feasible to find the minimum of a function through its derivative.\n",
    "* However, a neural network with many variables to differentiate and billions of weights, it becomes a more difficult matter and calculus will not work. \n",
    "\n",
    "The process for gradient descent works as follows:\n",
    "1. Compute $\\nabla C$\n",
    "2. Move in the direction of $-\\nabla C$. This step can be thought of that of having the most immediate effect in decreasing C. \n",
    "3. Repeat until minimum is reached.\n",
    "\n",
    "\n",
    "* We introduce $\\eta$ as the learning rate, which is just multiplies the change in $\\Delta v$, as to decrease the time spent to descend down the \"gradient valley\" in the each direction of the variable. \n",
    "    * A good $\\eta$ small enough not overshoot the minimum and large enough to as to descent down very slowly.\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  v \\rightarrow v' = v -\\eta \\nabla C.\n",
    "\\end{eqnarray}\n",
    "\n",
    "* Consider a function of two variables (3D), for step of gradient descent, if we can partially move in the direction towards the minimum of the cost function, we would be getting closer and closer towards reaching our goal. However, one problem is that depending one what random area you begin this descend would influence on what minimum you land in. There is a possibility that the minimum is not the global minimum.\n",
    "    * In our case, our random choice begin with the random initialization of weights and biases the network initially begins with.\n",
    "    * If $w$ are our starting weights (a column vector for a single neuron), then $-C(\\nabla w)$ will tell how to tune the weights and biases. In particular if the value is negative, then the parameter should be tuned up, and vise-versa. And the relevant magnitudes tell us the degree to which the weights should change. In other words, if for example, $\\nabla C(1,1) = [1,3]$ then this tell us that moving in the direction of the second parameter is three times more effective than than in the x parameter.  \n",
    "    * It is worth noting that many local minimums are of equal quality. In other words, there is a possibility that a many different set of weights produce the degree of $C(w, b)$.\n",
    "* To combat the problem of finding a local minimum, \n",
    "\n",
    "\n",
    "* Taking the gradient of the cost function, we obtain our update rules:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  w_k & \\rightarrow & w_k' = w_k-\\eta \\frac{\\partial C}{\\partial w_k}\\\\\n",
    "  b_l & \\rightarrow & b_l' = b_l-\\eta \\frac{\\partial C}{\\partial b_l}.\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "\n",
    "* Stochastic gradient descent is used to speed on computation. Instead of computing the gradient for EVERY training input x in order to make a decision on which step to make, we take a small random sample or \"mini-batch\" of the training set and average this sample of results. From a randomly sufficient sample, we can get a pretty good estimator. New random samples per update are selected every time.\n",
    "* So now, averaging over m samples, our equation gets modified to:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    w_k & \\rightarrow & w_k' = w_k-\\frac{\\eta}{m}\n",
    "  \\sum_j \\frac{\\partial C_{X_j}}{\\partial w_k} \\\\\n",
    "  b_l & \\rightarrow & b_l' = b_l-\\frac{\\eta}{m}\n",
    "  \\sum_j \\frac{\\partial C_{X_j}}{\\partial b_l}\n",
    "\\end{eqnarray}\n",
    "\n",
    "* One epoch means exhaustively training through all the inputs.\n",
    "* Sometimes $\\frac{1}{n}$ or $\\frac{1}{m}$ (the average) is omitted from the equations, when the number of training examples are not known in advance. Conceptually, this works out too, since it is equivalent to just rescaling $\\nabla$.\n",
    "\n",
    "###  Building the Network\n",
    "\n",
    "#### System Architecture\n",
    "* Each dense layer is randomly initialized with a weight matrix with values pretty close to zero. This is important to break symetry, since otherwise the each neuron would respond identically to that of its neigbore. The bias nuerons are on the other hand, initialized to zero, or set a custom. Initialization methods can be customarly set by `kernal_initializer` (another word for matrix for connection weights), and `bias_initializer`. \n",
    "* The input layer is also treated as as \"neuron\", but biases are omitted from this layer.\n",
    "* The entire network can be thought of as a large composite function. Each neuron itself takes in x amount of inputs and splits y amount of outputs, and the same could be said about the entire network.\n",
    "* The system architecture:\n",
    "\n",
    "![matrix nn representation](../assets/figures/matrix_arch.png)\n",
    "\n",
    "\n",
    "#### Activation Matrices\n",
    "* The activation for each neuron of every layer is determined as follows.\n",
    "\n",
    "![activation notation](../assets/figures/activation_notation.png)\n",
    "\n",
    "* To determine the activation for an arbitrary single neuron, we perform the dot product over a single row and a single element in the input column + a single bias from the bias column. The dot product between a matrix and a column vector computes of the activation in the next layer in a single go.\n",
    "\n",
    "#### Parameter Tuning\n",
    "* There is an art and some heuristics to go along with tuning the nuts and bolts of a neural net.\n",
    "* Some baseline performances:\n",
    "    * 10% by randomly guessing\n",
    "    * up 50% by selecting some features from the pixels, such as the average darkness.\n",
    "\n",
    "\n",
    "### What is Happening?\n",
    "* As a very rough approximation of what the network is doing we could imagine every sub-network attempting to classify certain parts of an image that what it out what it really is. For example, we could imagine the network individually classify the eyes, nose, ears, and hair of an image to determine whether the image has a face. Subsubnetworks can be further subcomposed into classifications such as eye brows, pupile, etc.\n",
    "    * The further you go into the network, the more abstract the classifications become.\n",
    "* Whether or not this actually happens really depends on the network, but this is not entirely the case. If we observe the weights matrix for each neuron, we can observe that the patterns are mostly just random.\n",
    "    * In fact if you feed it in random set of inputs, the neural network confidently produces a response rather than having a high degree of variability in the output layer like we might expect. Analogously, in convolutional neural networks (networks typically used in classifying images), a similiar things happens when you feed in an image of an elephant (for example), with some added noise, it completely gets the classification wrong with similar levels of confidence. \n",
    "    * For the perspective of the network, the entire universe is packed in to the training images that has been feed into it. Previous papers have shown however, that structured data learns more quickly than a set of randomly labeled images. This suggests some level of coherencey with the cost function.  \n",
    "\n",
    "![image]()\n",
    "\n",
    "### Concluding Thoughts\n",
    "* Neural nets have been shown to be more performant than SVMs for this particular job.\n",
    "* A sophisticated algorithm ≤ simple learning algorithm + good training data.\n",
    "* Deep neural networks have two or more hidden layers.\n",
    "    * For many complex problems deep neural networks out-perform shallow networks.\n",
    "\n",
    "\n",
    "## Chapter 2 - How the Back Propagation Algorithm Works\n",
    "\n",
    "### Recap\n",
    "* Back propagation is an algorithm an efficient way to tune the biases and weights by computing the gradient.\n",
    "* The gradient $-\\nabla(w, b)$ for a given set of weights and biases produce the a column vector of values that can be thought of encoding the level of sensitivity against the described cost function.\n",
    "* Obtain the total cost of a network, we sum the square differences with output of in the final layer to the output we want, and we average all of these costs with every single training example. This way, the total cost remains independent of the training size (we do not want a larger cost just because we have more training examples).\n",
    "* A single step of gradient decent in the back propagation algorithm will nudge some changes in the weights, and to compute a single step of gradient descent, all of the training examples (or a subset of them) must be iterated through. \n",
    "\n",
    "\n",
    "### Intuition\n",
    "\n",
    "![back prop 1](../assets/images/back_prop1.png)\n",
    "\n",
    "* When the input weights and biases propagate through the network, and the final layer of neurons are activated, that is when we can begin our descent back through the network to update the weights.\n",
    "* For example, suppose a 2 was to be classified in the network. A few things can be done in the second to last layer to make the classification more \"correct\". \n",
    "    * In particular there are 3 avenues of influences: changes to the weights, biases, or activations. \n",
    "    * ![back prop 2](../assets/images/back_prop2.png)\n",
    "    * Suppose that in the image above, .2 was final layer activation for classifying a 2, to decrease the cost for this particular training example and for this final neuron in the output layer, it would make sense to:\n",
    "        * Increase the biases or \n",
    "        * Increase the weights that are positive or decrease the weights that are negative\n",
    "    * However, every other neuron in the final layer has its own idea on how the weights should change that would benefit itself the most. So what is done is that for a single training example, all of the desired changes are summed together.\n",
    "    * Now using this same single training example, this procedure can become applied to the previous layers recursively.\n",
    "    * This process is repeated for every training example. So every training image (or subset) produces a list of change it would like to make to the weights and biases of the network. To apply these changes, the stacked weights averaged and finally applied. These averaged changes are what define $-\\nabla C(w,b)$\n",
    "    * We can make any changes to the weights and biases, only every weight and bias is different in that a change to themselves will influence a change in the final layer a bit differently. In other words, the derivatives have different magnitudes. We can track how a small change in $w_{jk}^l$ can propagate through the network (across all paths, hence the summation) as follows:\n",
    "    \n",
    "\\begin{eqnarray} \n",
    "  \\Delta C \\approx \\sum_{mnp\\ldots q} \\frac{\\partial C}{\\partial a^L_m} \n",
    "  \\frac{\\partial a^L_m}{\\partial a^{L-1}_n}\n",
    "  \\frac{\\partial a^{L-1}_n}{\\partial a^{L-2}_p} \\ldots\n",
    "  \\frac{\\partial a^{l+1}_q}{\\partial a^l_j} \n",
    "  \\frac{\\partial a^l_j}{\\partial w^l_{jk}} \\Delta w^l_{jk},\n",
    "\\tag{52}\\end{eqnarray}\n",
    "\n",
    "\n",
    "### Matrix Based Notation\n",
    "\n",
    "To describe the layers of the network, we use the notation:\n",
    "\n",
    "![notation](../assets/figures/notation.png)\n",
    "\n",
    "* So reading from left to right (ljk) it goes, layer -> row -> column\n",
    "    * Where l denotes the lth matrix (by layer), the jth row would return a list of weights of the jth neuron in the layer, and the kth weight would refer to that specific weight.\n",
    "\n",
    "* The activation function is determined as follows:\n",
    "\n",
    "\\begin{eqnarray} \n",
    "  a^{l}_j = \\sigma\\left( \\sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \\right),\n",
    "\\end{eqnarray}\n",
    "\n",
    "or in vectorized notation\n",
    "\n",
    "\\begin{eqnarray} \n",
    "  a^{l} = \\sigma(w^l a^{l-1}+b^l).\n",
    "\\end{eqnarray}\n",
    "\n",
    "simplifying this further\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    z^l \\equiv w^l a^{l-1}+b^l\n",
    "\\end{eqnarray}\n",
    "\n",
    "and so the activation is finally described as \n",
    "\n",
    "\\begin{eqnarray}\n",
    "    z^l \\equiv w^l a^{l-1}+b^l\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "### Assumptions about the Cost Function\n",
    "\n",
    "1. The cost function is written as the average over the the individual cost function $C_x$ over a single training example: $C = \\frac{1}{n} \\sum_x C_x$. For a single training example, the cost is written as $C_x =\n",
    "\\frac{1}{2} \\|y-a^L \\|^2$, which is defined as the squared difference between the response and our goal. The reason why it is ok to multiple the cost function by a constant everything gets affected by the same amount, and so the relative change between different costs remains 0. And constant could have been added, but $\\frac{1}{2}$ is included for mathematical simplification when the derivative is taken.\n",
    "    * The reason why this is important to note is because back propagation is specific towards a single training example (we are basically ignoring the summation here). If we can solve this for a single training example, then it becomes very easy to compute it for the rest, because in the end all the changes to the weights would be averaged out.\n",
    "    * The notation for $C_x$ will be dropped to just $C$ for the remainder of the chapter.\n",
    "2. The cost function can be written as a function from the output layer of the network alone: $C = C(a^L)$.\n",
    "    * The question as to why y is not included in the function is because it is fixed, and has no bearing on the cost. In other words, it is not a parameter that can be learned. It just defines the function for explicitly.\n",
    "\n",
    "\n",
    "### Four Fundamental Equations Behind the Back Propagation\n",
    "\n",
    "* First, we define _error_ as the change needed for a weight to reduce the cost function. \n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j).\n",
    "\\end{eqnarray}\n",
    "\n",
    "* $\\frac{\\partial C}{\\partial a^L_j}$ describes the rate of change of a cost function with respect to the jth activation function. That is, it describes how much C changes with respect to how $a^L_j$ changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "258px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
