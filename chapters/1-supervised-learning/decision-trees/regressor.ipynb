{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regression\n",
    "\n",
    "A naive `CART` implementation.\n",
    "\n",
    "Before reading this chapter, take a look at [Decision Tree Classification](), which introduces the concept of decision trees better.\n",
    "\n",
    "Structurely, a regression tree are constructed in the same way as a classification tree. There are however, two key differences:\n",
    "\n",
    "1. Because we can now attribute error to distance, the **quality** of splitting is now measured by variance of two children. Variance is defined as $Var(X) = E[(X-\\mu)^2]$. Therefore, an optimial split is defined having the lowest possible variance between the other combinations of splits.\n",
    "    1. Note: the difference between MSE and variance is that variance measures the dispertion of values, while MSE measures the quality of an estimator, or in other words, how different the values of the estimator and actual values are.\n",
    "2. Terminal nodes (or leaf nodes) that reveal the prediction is no longer a majority vote, but the average value instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Suppose we had the following information about housing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>n_bedrooms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>semi</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>detached</td>\n",
       "      <td>2</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>detached</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>semi</td>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>semi</td>\n",
       "      <td>4</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type  n_bedrooms  price\n",
       "0      semi           3    600\n",
       "1  detached           2    700\n",
       "2  detached           3    800\n",
       "3      semi           2    400\n",
       "4      semi           4    700"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'type': ['semi', 'detached', 'detached', 'semi', 'semi'],\n",
    "                   'n_bedrooms': [3, 2, 3, 2, 4],\n",
    "                   'price': [600, 700, 800, 400, 700]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the split functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _variance(targets):\n",
    "    if targets.size == 0:\n",
    "        return 0\n",
    "    return np.var(targets)\n",
    "\n",
    "\n",
    "def _weighted_variance(groups):\n",
    "    \"\"\"\n",
    "    obj: measures the weighted variance (impurity) after a split. this is effective \n",
    "    the sum of weighted variances, where the weight is defined quality of elements\n",
    "    within each group\n",
    "    :param groups: List[List[]] - [i] - child, [i][j] - target value at child i\n",
    "    :return: - float\n",
    "    \"\"\"\n",
    "    total = sum(len(group) for group in groups)\n",
    "\n",
    "    def single_wv(group):\n",
    "        weight = len(group) / float(total)\n",
    "        return weight * _variance(group)\n",
    "    \n",
    "    return reduce(lambda g1, g2: single_wv(g1) + single_wv(g2), groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_variance(np.array([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_weighted_variance([np.array([1, 2, 3]), np.array([1, 2])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our choice of split by selecting the minimum weighted variance between features. In other words, compare the weighted variance between every combination of features and select the split the returns the smallest value. This is the same as selecting a split that returns the overall smallest spread between values - or the group of features that are overall, the closest together.\n",
    "\n",
    "This has the effect of grouping two together features, or making a decision on the basis that the information gained from the split is defined by being more closer together, forming more tightly knited groups as a consequence. The leaf nodes that determine the final predictions of the tree will as a result carry minimum variance and so the target mean will be maximimally explained.\n",
    "\n",
    "In our case, the comparisons we make is based on the combinations between the number of bedrooms and type of home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature: type\n",
      "value: semi\n",
      "split_variance: 10333.333333333334\n",
      "\n",
      "feature: type\n",
      "value: detached\n",
      "split_variance: 10333.333333333334\n",
      "\n",
      "feature: n_bedrooms\n",
      "value: 3\n",
      "split_variance: 16000.0\n",
      "\n",
      "feature: n_bedrooms\n",
      "value: 2\n",
      "split_variance: 13000.0\n",
      "\n",
      "feature: n_bedrooms\n",
      "value: 4\n",
      "split_variance: 17500.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def split_variance(df, feature, value, target):\n",
    "    is_value = df[feature] == value\n",
    "    is_not_value = df[feature] != value\n",
    "    child_split = [np.array(df[is_value][target].tolist()),\n",
    "                   np.array(df[is_not_value][target].tolist())]\n",
    "    return _weighted_variance(child_split)\n",
    "\n",
    "\n",
    "split_type = df['type'].unique().tolist()\n",
    "split_bedrooms = df['n_bedrooms'].unique().tolist()\n",
    "split_type = [('type', elm) for elm in split_type]\n",
    "split_bedrooms = [('n_bedrooms', elm) for elm in split_bedrooms]\n",
    "\n",
    "for feature, value in split_type:\n",
    "    print(f'feature: {feature}')\n",
    "    print(f'value: {value}')\n",
    "    sv = split_variance(df, feature, value, 'price')\n",
    "    print(f'split_variance: {sv}', end='\\n\\n')\n",
    "\n",
    "\n",
    "for feature, value in split_bedrooms:\n",
    "    print(f'feature: {feature}')\n",
    "    print(f'value: {value}')\n",
    "    sv = split_variance(df, feature, value, 'price')\n",
    "    print(f'split_variance: {sv}', end='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that splitting on 'semi' first proves to be optimal. Then following this same strategy again, splitting next on bedroom (3) proves me to be the next optimal split.\n",
    "\n",
    "Finally, we average out the remaining values within the dataframe to indicate the final predictive value.\n",
    "\n",
    "![](../../../assets/tree_based_algorithms/sample_decision_tree2.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "class Impurity:\n",
    "    @staticmethod\n",
    "    def weighted_variance(groups):\n",
    "        \"\"\"\n",
    "        obj: measures the weighted variance (impurity) after a split. this is effectively\n",
    "        the sum of weighted variances, where the weight is defined quality of elements\n",
    "        within each group\n",
    "        :param groups: List[List[]] - [i] - child, [i][j] - target value at child i\n",
    "        :return: - float\n",
    "        \"\"\"\n",
    "        total = sum(len(group) for group in groups)\n",
    "\n",
    "        def single_wv(group):\n",
    "            weight = len(group) / float(total)\n",
    "            return weight * Impurity._variance(group)\n",
    "\n",
    "        return reduce(lambda g1, g2: single_wv(g1) + single_wv(g2), groups)\n",
    "\n",
    "    @staticmethod\n",
    "    def _variance(targets):\n",
    "        if targets.size == 0:\n",
    "            return 0\n",
    "        return np.var(targets)\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, data, feature, impurity,\n",
    "                 left=None, right=None, leaf=None):\n",
    "        \"\"\"\n",
    "        :param data: pd.DataFrame - subset data by a particular feature group\n",
    "        :param feature: str - feature `data` was grouped by\n",
    "        :param impurity: float - metric value that was made for the optimal decision of the split\n",
    "        :param left: Node - pointer to left child\n",
    "        :param right: Node - pointer to right child\n",
    "        :param leaf: int - average target value represented by the leaf node\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.feature = feature\n",
    "        self.impurity = impurity\n",
    "        self.left, self.right = left, right\n",
    "        self.leaf = leaf\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.leaf is not None and self.left is None and self.right is None\n",
    "\n",
    "    def print(self, delimit):\n",
    "        md_table = None\n",
    "        if self.data is not None:\n",
    "            md_table = tabulate(self.data.head(10), headers='keys', tablefmt='pipe')\n",
    "            md_table = '\\n'.join([delimit + row for row in md_table.split('\\n')])\n",
    "        feature = delimit + 'feature: ' + self.feature if self.feature else ''\n",
    "        impurity = delimit + 'impurity: ' + str(self.impurity) if self.impurity else ''\n",
    "        leaf = delimit + 'leaf: ' + str(self.leaf) if self.leaf else ''\n",
    "        tmp_iter = [md_table, feature, impurity, leaf]\n",
    "        print('\\n'.join([elm for elm in tmp_iter if elm]))\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    @staticmethod\n",
    "    def train(data, max_depth, min_size, y_label='y'):\n",
    "        \"\"\"\n",
    "        obj: build or \"train\" the decision tree\n",
    "        :param data: pd.DataFrame - training data from its entry point\n",
    "        :param max_depth: int - constraint for the maximum depth of tree\n",
    "        :param min_size: int - constrain for the minimum number of rows per row\n",
    "        :param y_label: str - target label into `data`\n",
    "        :return: Node - root of the decision tree\n",
    "        \"\"\"\n",
    "        def dfs(data, depth):\n",
    "            # checking terminating conditions (sufficient data, and depth)\n",
    "            if len(data) == 0 or len(data) <= min_size or depth >= max_depth:\n",
    "                return Node(data, None, None, leaf=DecisionTree._predict(data, y_label))\n",
    "            # otherwise we are safe to split again\n",
    "            else:\n",
    "                # build the new node in the stack and recurse to the next level\n",
    "                # form the binary connections on return\n",
    "                best_split_info = DecisionTree._get_best_split(data, y_label)\n",
    "                cur_node = Node(data, best_split_info['col_name'], best_split_info['impurity'])\n",
    "                cur_node.left = dfs(best_split_info['left'], depth + 1)\n",
    "                cur_node.right = dfs(best_split_info['right'], depth + 1)\n",
    "                return cur_node\n",
    "        return dfs(data, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize_tree(root):\n",
    "        def dfs(node, tab_count):\n",
    "            delimit = '\\t' * tab_count\n",
    "            node.print(delimit)\n",
    "            if not node.is_leaf():\n",
    "                dfs(node.left, tab_count + 1)\n",
    "                dfs(node.right, tab_count + 1)\n",
    "        dfs(root, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_best_split(data, y_label):\n",
    "        \"\"\"\n",
    "        obj: identify the best attribute to split on ``data``\n",
    "        :param data: pd.DataFrame - subset data\n",
    "        :param y_label: str - label for output in data\n",
    "        :return: dict - the score, left, right, and column for optimal split\n",
    "        \"\"\"\n",
    "        def get_best_score(one_rests):\n",
    "            # note: top score is the minimum score\n",
    "            top_score, top_i = sys.maxsize, None\n",
    "            for i, one_rest in enumerate(one_rests):\n",
    "                impurity = Impurity.weighted_variance([one_rest[0][y_label],\n",
    "                                                       one_rest[1][y_label]])\n",
    "                if impurity < top_score:\n",
    "                    top_score, top_i = impurity, i\n",
    "            return top_score, top_i\n",
    "\n",
    "        features = DecisionTree.get_features(data, y_label)\n",
    "        g_top_score, g_top_one_rest, g_top_col = sys.maxsize, None, None\n",
    "\n",
    "        # get all the possible binary splits for a particular attribute\n",
    "        for col_name in features:\n",
    "            one_rests = DecisionTree._get_all_splits(data, col_name)\n",
    "            top_score, top_i = get_best_score(one_rests)\n",
    "            if g_top_score > top_score:\n",
    "                g_top_score, g_top_one_rest, g_top_col = top_score, one_rests[top_i], col_name\n",
    "\n",
    "        return {'impurity': g_top_score, 'left': g_top_one_rest[0],\n",
    "                'right': g_top_one_rest[1], 'col_name': g_top_col}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_features(df, y_label):\n",
    "        \"\"\"\n",
    "        obj: return the features of the dataframe\n",
    "        \"\"\"\n",
    "        c_names = np.array(list(df))\n",
    "        features = c_names[c_names != y_label]\n",
    "        return features\n",
    "\n",
    "    @staticmethod\n",
    "    def _predict(data, y_label):\n",
    "        \"\"\"\n",
    "        obj: determine leaf node value of tree\n",
    "        \"\"\"\n",
    "        return np.mean(data[y_label])\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_all_splits(data, by):\n",
    "        \"\"\"\n",
    "        objective: split data by all unique attributes within a feature\n",
    "        :param data: pd.DataFrame - subset data\n",
    "        :param by: str - column to group by\n",
    "        :return: [(pd.DataFrame, pd.DataFrame)] - (one, rest) data frames. this can\n",
    "        also be interpreted as (left, right) where left = unique, right = remainder\n",
    "        \"\"\"\n",
    "        groups = data[by].unique()\n",
    "        one_rest = []\n",
    "        for elm in groups:\n",
    "            one, rest = DecisionTree._partition(data, by, elm)\n",
    "            one_rest.append((one, rest))\n",
    "        return one_rest\n",
    "\n",
    "    @staticmethod\n",
    "    def _partition(data, feature, value):\n",
    "        \"\"\"\n",
    "        obj: partition dataframe into two, using one vs rest approach\n",
    "        :param data: pd.Dataframe - data\n",
    "        :param feature: str - column name\n",
    "        :param value: int - element into column to split dataframe by\n",
    "        :return: [d1, df2] - (one, rest) datafrmaes\n",
    "        \"\"\"\n",
    "        mask = data[feature] == value\n",
    "        return data[mask], data[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | type     |   n_bedrooms |   price |\n",
      "|---:|:---------|-------------:|--------:|\n",
      "|  0 | semi     |            3 |     600 |\n",
      "|  1 | detached |            2 |     700 |\n",
      "|  2 | detached |            3 |     800 |\n",
      "|  3 | semi     |            2 |     400 |\n",
      "|  4 | semi     |            4 |     700 |\n",
      "feature: type\n",
      "impurity: 10333.333333333334\n",
      "\t|    | type   |   n_bedrooms |   price |\n",
      "\t|---:|:-------|-------------:|--------:|\n",
      "\t|  0 | semi   |            3 |     600 |\n",
      "\t|  3 | semi   |            2 |     400 |\n",
      "\t|  4 | semi   |            4 |     700 |\n",
      "\tfeature: n_bedrooms\n",
      "\timpurity: 1666.6666666666665\n",
      "\t\t|    | type   |   n_bedrooms |   price |\n",
      "\t\t|---:|:-------|-------------:|--------:|\n",
      "\t\t|  3 | semi   |            2 |     400 |\n",
      "\t\tleaf: 400.0\n",
      "\t\t|    | type   |   n_bedrooms |   price |\n",
      "\t\t|---:|:-------|-------------:|--------:|\n",
      "\t\t|  0 | semi   |            3 |     600 |\n",
      "\t\t|  4 | semi   |            4 |     700 |\n",
      "\t\tleaf: 650.0\n",
      "\t|    | type     |   n_bedrooms |   price |\n",
      "\t|---:|:---------|-------------:|--------:|\n",
      "\t|  1 | detached |            2 |     700 |\n",
      "\t|  2 | detached |            3 |     800 |\n",
      "\tleaf: 750.0\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'type': ['semi', 'detached', 'detached', 'semi', 'semi'],\n",
    "                   'n_bedrooms': [3, 2, 3, 2, 4],\n",
    "                   'price': [600, 700, 800, 400, 700]})\n",
    "tree = DecisionTree.train(df, 3, 2, 'price')\n",
    "DecisionTree.visualize_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this matches the same tree structure as in the diagram above.\n",
    "\n",
    "For the purposes of demonstration, we are for example ok with working with a single row of data (which we shouldn't be in the real world), we can change the parameters as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | type     |   n_bedrooms |   price |\n",
      "|---:|:---------|-------------:|--------:|\n",
      "|  0 | semi     |            3 |     600 |\n",
      "|  1 | detached |            2 |     700 |\n",
      "|  2 | detached |            3 |     800 |\n",
      "|  3 | semi     |            2 |     400 |\n",
      "|  4 | semi     |            4 |     700 |\n",
      "feature: type\n",
      "impurity: 10333.333333333334\n",
      "\t|    | type   |   n_bedrooms |   price |\n",
      "\t|---:|:-------|-------------:|--------:|\n",
      "\t|  0 | semi   |            3 |     600 |\n",
      "\t|  3 | semi   |            2 |     400 |\n",
      "\t|  4 | semi   |            4 |     700 |\n",
      "\tfeature: n_bedrooms\n",
      "\timpurity: 1666.6666666666665\n",
      "\t\t|    | type   |   n_bedrooms |   price |\n",
      "\t\t|---:|:-------|-------------:|--------:|\n",
      "\t\t|  3 | semi   |            2 |     400 |\n",
      "\t\tleaf: 400.0\n",
      "\t\t|    | type   |   n_bedrooms |   price |\n",
      "\t\t|---:|:-------|-------------:|--------:|\n",
      "\t\t|  0 | semi   |            3 |     600 |\n",
      "\t\t|  4 | semi   |            4 |     700 |\n",
      "\t\tleaf: 650.0\n",
      "\t|    | type     |   n_bedrooms |   price |\n",
      "\t|---:|:---------|-------------:|--------:|\n",
      "\t|  1 | detached |            2 |     700 |\n",
      "\t|  2 | detached |            3 |     800 |\n",
      "\tfeature: n_bedrooms\n",
      "\t\t|    | type     |   n_bedrooms |   price |\n",
      "\t\t|---:|:---------|-------------:|--------:|\n",
      "\t\t|  1 | detached |            2 |     700 |\n",
      "\t\tleaf: 700.0\n",
      "\t\t|    | type     |   n_bedrooms |   price |\n",
      "\t\t|---:|:---------|-------------:|--------:|\n",
      "\t\t|  2 | detached |            3 |     800 |\n",
      "\t\tleaf: 800.0\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTree.train(df, 3, 1, 'price')\n",
    "DecisionTree.visualize_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorportating Random Forests\n",
    "\n",
    "A Random Forest incorporates the combination two critical things:\n",
    "1. Many decision trees.\n",
    "2. Each decision tree randomly subsamples features within each node in a tree.\n",
    "\n",
    "A _regression forest_ which is a kind of random forest, predicts the final value by incorporating all the predictions from its seperate decision trees using a metric just as average, or weighted average. \n",
    "\n",
    "\n",
    "[UPDATE!????] In a regression forest, all fields must be numerical. (then why decision used string type?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_split=3)\n",
    "regressor.fit(X_train, y_train)\n",
    "predictions = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can identify the following properties used within our random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees in forest: 100\n",
      "The maximum depth of the tree is: 10. This means that no more than 10splits in the data were used.\n",
      "Every node in the tree contained at least 3 samples within its own data subset.\n",
      "The minimum weighted fraction for a leaf node is 0.0. This is the percentage ofsamples required (overall the samples) to be deemed a leaf node.\n",
      "The criterion used measure quality of each split was mse. Like demonstrated above, themse criterion is equivalent to greedily reducting the variance.The split that minimizes the variance ensures that the target data are most closely aligned together.\n",
      "When identifying the best split, the maximum features that are randomly considered at auto. \n",
      "The additional constraint for the maximum number of leaf nodes is None. This is means thatthere is no constraint (unlimited number of leaf nodes).\n",
      "A split will be enforced if the difference between the impurity of the previous split and the current split is lessthan 0.0.\n",
      "To both fit and predict the model, None were run in parallel.\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of trees in forest: {regressor.n_estimators}')\n",
    "print(f'The maximum depth of the tree is: {regressor.max_depth}. This means that no more than {regressor.max_depth}' \n",
    "      f'splits in the data were used.')\n",
    "print(f'Every node in the tree contained at least {regressor.min_samples_split} samples within its own data subset.')\n",
    "print(f'The minimum weighted fraction for a leaf node is {regressor.min_weight_fraction_leaf}. This is the percentage of' \n",
    "      f'samples required (overall the samples) to be deemed a leaf node.')\n",
    "print(f'The criterion used measure quality of each split was {regressor.criterion}. Like demonstrated above, the' \n",
    "      f'mse criterion is equivalent to greedily reducting the variance.'\n",
    "      f'The split that minimizes the variance ensures that the target data are most closely aligned together.')\n",
    "print(f'When identifying the best split, the maximum features that are randomly considered at {regressor.max_features}. ')\n",
    "print(f'The additional constraint for the maximum number of leaf nodes is {regressor.max_leaf_nodes}. This is means that'\n",
    "      f'there is no constraint (unlimited number of leaf nodes).')\n",
    "print(f'A split will be enforced if the difference between the impurity of the previous split and the current split is less'\n",
    "      f'than {regressor.min_impurity_decrease}.')\n",
    "print(f'To both fit and predict the model, {regressor.n_jobs} were run in parallel.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
