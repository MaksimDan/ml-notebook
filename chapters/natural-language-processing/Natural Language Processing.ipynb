{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the 20 Newsgroups Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR', 'description'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# explorating options\n",
    "groups = fetch_20newsgroups()\n",
    "groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_groups: 20\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# the news groups\n",
    "print('n_groups:', len(groups.target_names))\n",
    "print(groups.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4, 4, ..., 3, 1, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# integer encoding of the newsgroup\n",
    "groups.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_posts 11314\n",
      "\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample data\n",
    "print('n_posts', len(groups.data), end='\\n\\n')\n",
    "print(groups.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see what target this first data element belongs to\n",
    "groups.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rec.autos'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which belongs to this group\n",
    "groups.target_names[groups.target[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "g = sns.countplot(groups.target, color='blue')\n",
    "plt.title('Distribution of Posts by Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "### Option 1: Count Vectorizer\n",
    "* Count vectorizer basically packages in counting the distribution of works in a corpus a brief description of some of less obvious parameters and their meaning\n",
    "* `ngram_range`: the minimum and maximum n_grams considered:\n",
    "    * ex: dan went to the park\n",
    "        * `(1,2)`: `[dan, went, to, the, park]`, `[dan went, went to, to the, the park]`\n",
    "* `stop_words`: filter common contextualess words\n",
    "* `max_features`: consider only a limited number of features. In our case, the features are the words, and so if we limit to 500 features for example, we would only be considering 500 of the most common words, and the other words will be left out.\n",
    "* `binary`: sets nonzero counts to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ago', 'agree', 'al', 'american', 'andrew', 'answer', 'anybody', 'apple', 'application', 'apr', 'april', 'area', 'argument', 'armenian', 'armenians']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1,1),\n",
    "                     stop_words=\"english\",\n",
    "                     lowercase=True,\n",
    "                     max_features=500,\n",
    "                     binary=False)\n",
    "\n",
    "# fit transformed takes in a corpus of documents\n",
    "transformed = cv.fit_transform(groups.data)\n",
    "\n",
    "# view some of the features\n",
    "print(cv.get_feature_names()[50:65])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: TF-IDF Vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01000100b', '010235', '0109', '011', '0111', '0114', '0123456789', '0126', '013', '013651', '014', '015', '01580', '01730', '01752']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidfv = TfidfVectorizer(sublinear_tf=True, \n",
    "                                   max_df=0.5, \n",
    "                                   stop_words='english', \n",
    "                                   max_features=40000)\n",
    "transformed = tfidfv.fit_transform(groups.data)\n",
    "\n",
    "# view some of the features\n",
    "print(tfidfv.get_feature_names()[50:65])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements with Data Preprocessing\n",
    "\n",
    "* Notice that some of the features are unfiltered. For example, we have names and numbers - which are somewhat arbitrary.\n",
    "* Below we apply some filtering techniques by lemmatizing (finding the appropriate root word e.g. runs -> run), ignoring numbers and names.\n",
    "* We can alternatively use nouns or other parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     /Users/danmaksimovich/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/danmaksimovich/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/danmaksimovich/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "['able', 'access', 'act', 'action', 'actually', 'address', 'advance', 'ago', 'agree', 'air']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('names')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import names\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "NAMES = set(names.words())\n",
    "\n",
    "def my_filter(post):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    f1 = [word.lower() for word in nltk.tokenize.word_tokenize(post)]\n",
    "    f2 = [word for word in f1 if word.isalpha() and word not in NAMES]\n",
    "    return ' '.join(lemmatizer.lemmatize(word) for word in f2)\n",
    "\n",
    "\n",
    "cleaned_posts = []\n",
    "for post in groups.data:\n",
    "    cleaned_posts.append(my_filter(post))\n",
    "    \n",
    "transformed = cv.fit_transform(cleaned_posts)\n",
    "print(cv.get_feature_names()[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Exactly does `fit_transform` do in the context of `CountVectorizer`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Is a `fit` followed by a `transform`. `fit` standardizes the data (mean=0, sd=1), and `transform` is what convert this information to a document term matrix.\n",
    "* `fit_transform` combines these two operators together and learns the vocabulary dictionary and return term-document matrix. So basically what we get back is a matrix represents the word frequency for every feature in `cv.get_feature_names()`. The rows are the documents in order, and the columns are the frequency of these terms.\n",
    "* It's a very concrete way of summarizing the distribution of features per document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_posts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "258px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
